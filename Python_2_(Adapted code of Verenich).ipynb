{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis Python 2 (voor zolang het nog ondersteund is)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r0664975/Next-event-prediction-in-Learning-analytics/blob/main/Python_2_(Adapted%20code%20of%20Verenich).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okh8UhG6lG7R"
      },
      "source": [
        "###Locating drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AHXFxVIfjZ9",
        "outputId": "32b41a84-4afa-48a0-937f-57025d8cd6e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOvLCw2_k91m"
      },
      "source": [
        "###GPU control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWc4FADuR_1j",
        "outputId": "d9c1d98d-f12d-43dc-9654-5df1d7b5ad00"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5eC6FbllC6b"
      },
      "source": [
        "##**Code: train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKSZ1FWZSN9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d368af60-0245-4b3f-8cec-616acb96ab19"
      },
      "source": [
        "'''\n",
        "this script trains an LSTM model on one of the data files in the data folder of\n",
        "this repository. the input file can be changed to another file from the data folder\n",
        "by changing its name in line 46.\n",
        " \n",
        "it is recommended to run this script on GPU, as recurrent networks are quite \n",
        "computationally intensive.\n",
        " \n",
        "Author: Niek Tax\n",
        "'''\n",
        "!pip install unicodecsv\n",
        " \n",
        "from __future__ import print_function, division\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
        "from keras.layers import Input\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.optimizers import Nadam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from collections import Counter\n",
        "import unicodecsv\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import copy\n",
        "import csv\n",
        "import time\n",
        "from itertools import izip\n",
        "from datetime import datetime\n",
        "from math import log\n",
        " \n",
        "print(\"imports done\")\n",
        " \n",
        "eventlog = \"edxpddf.csv\"\n",
        " \n",
        "########################################################################################\n",
        "#\n",
        "# this part of the code opens the file, reads it into three following variables\n",
        "#\n",
        " \n",
        "lines = [] #these are all the activity seq\n",
        "timeseqs = [] #time sequences (differences between two events)\n",
        "timeseqs2 = [] #time sequences (differences between the current and first)\n",
        " \n",
        "#helper variables\n",
        "lastcase = ''\n",
        "line = ''\n",
        "firstLine = True\n",
        "times = []\n",
        "times2 = []\n",
        "numlines = 0\n",
        "casestarttime = None\n",
        "lasteventtime = None\n",
        " \n",
        "\n",
        "csvfile = open('/content/drive/MyDrive/content/data/%s' % eventlog, 'r')\n",
        "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "next(spamreader, None)  # skip the headers\n",
        "ascii_offset = 161\n",
        " \n",
        "for row in spamreader: #the rows are \"CaseID,ActivityID,CompleteTimestamp\"\n",
        "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\") #creates a datetime object from row[2]\n",
        "    if row[0]!=lastcase:  #'lastcase' is to save the last executed case for the loop\n",
        "        casestarttime = t\n",
        "        lasteventtime = t\n",
        "        lastcase = row[0]\n",
        "        if not firstLine:\n",
        "            lines.append(line)\n",
        "            timeseqs.append(times)\n",
        "            timeseqs2.append(times2)\n",
        "        line = ''\n",
        "        times = []\n",
        "        times2 = []\n",
        "        numlines+=1\n",
        "    line+=unichr(int(row[1])+ascii_offset)\n",
        "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
        "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
        "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
        "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
        "    times.append(timediff)\n",
        "    times2.append(timediff2)\n",
        "    lasteventtime = t\n",
        "    firstLine = False\n",
        " \n",
        "# add last case\n",
        "lines.append(line)\n",
        "timeseqs.append(times)\n",
        "timeseqs2.append(times2)\n",
        "numlines+=1\n",
        " \n",
        "########################################\n",
        " \n",
        "divisor = np.mean([item for sublist in timeseqs for item in sublist]) #average time between events\n",
        "print('divisor: {}'.format(divisor))\n",
        "divisor2 = np.mean([item for sublist in timeseqs2 for item in sublist]) #average time between current and first events\n",
        "print('divisor2: {}'.format(divisor2))\n",
        " \n",
        " \n",
        " \n",
        "#########################################################################################################\n",
        " \n",
        "# separate training data into 3 parts\n",
        " \n",
        "elems_per_fold = int(round(numlines/3))\n",
        "fold1 = lines[:elems_per_fold]\n",
        "fold1_t = timeseqs[:elems_per_fold]\n",
        "fold1_t2 = timeseqs2[:elems_per_fold]\n",
        " \n",
        "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
        " \n",
        "fold3 = lines[2*elems_per_fold:]\n",
        "fold3_t = timeseqs[2*elems_per_fold:]\n",
        "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
        " \n",
        "#leave away fold3 for now\n",
        "lines = fold1 + fold2\n",
        "lines_t = fold1_t + fold2_t\n",
        "lines_t2 = fold1_t2 + fold2_t2\n",
        " \n",
        "step = 1\n",
        "sentences = []\n",
        "softness = 0\n",
        "next_chars = []\n",
        "lines = map(lambda x: x+'!',lines) #put delimiter symbol\n",
        "maxlen = max(map(lambda x: len(x),lines)) #find maximum line size\n",
        "#We are adapting the things that were made with this code:\n",
        "#   r86 line+=unichr(int(row[1])+ascii_offset) This is creates a Unicode (string) from the activity\n",
        " \n",
        "# next lines here to get all possible characters for events and annotate them with numbers\n",
        "chars = map(lambda x: set(x),lines)\n",
        "chars = list(set().union(*chars))\n",
        "chars.sort()\n",
        "target_chars = copy.copy(chars)\n",
        "chars.remove('!')\n",
        "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
        "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
        "print(\"char_indices\")\n",
        "print(char_indices)\n",
        "print(\"indices_char\")\n",
        "print(indices_char)\n",
        "print(\"target_char_indices\")\n",
        "print(target_char_indices)\n",
        "print(\"target_indices_char\")\n",
        "print(target_indices_char)\n",
        " \n",
        "csvfile = open('/content/drive/MyDrive/content/data/%s' % eventlog, 'r')\n",
        "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "next(spamreader, None)  # skip the headers\n",
        "lastcase = ''\n",
        "line = ''\n",
        "firstLine = True\n",
        "lines = []\n",
        "timeseqs = []\n",
        "timeseqs2 = []\n",
        "timeseqs3 = []\n",
        "timeseqs4 = []\n",
        "times = []\n",
        "times2 = []\n",
        "times3 = []\n",
        "times4 = []\n",
        "numlines = 0\n",
        "casestarttime = None\n",
        "lasteventtime = None\n",
        "for row in spamreader:\n",
        "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
        "    if row[0]!=lastcase:\n",
        "        casestarttime = t\n",
        "        lasteventtime = t\n",
        "        lastcase = row[0]\n",
        "        if not firstLine:\n",
        "            lines.append(line)\n",
        "            timeseqs.append(times)\n",
        "            timeseqs2.append(times2)\n",
        "            timeseqs3.append(times3)\n",
        "            timeseqs4.append(times4)\n",
        "        line = ''\n",
        "        times = []\n",
        "        times2 = []\n",
        "        times3 = []\n",
        "        times4 = []\n",
        "        numlines+=1\n",
        "    line+=unichr(int(row[1])+ascii_offset)\n",
        "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
        "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
        "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
        "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
        "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
        "    timediff3 = timesincemidnight.seconds #this leaves only time even occured after midnight\n",
        "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\n",
        "    times.append(timediff)\n",
        "    times2.append(timediff2)\n",
        "    times3.append(timediff3)\n",
        "    times4.append(timediff4)\n",
        "    lasteventtime = t\n",
        "    firstLine = False\n",
        " \n",
        "# add last case\n",
        "lines.append(line)\n",
        "timeseqs.append(times)\n",
        "timeseqs2.append(times2)\n",
        "timeseqs3.append(times3)\n",
        "timeseqs4.append(times4)\n",
        "numlines+=1\n",
        " \n",
        "elems_per_fold = int(round(numlines/3))\n",
        "fold1 = lines[:elems_per_fold]\n",
        "fold1_t = timeseqs[:elems_per_fold]\n",
        "fold1_t2 = timeseqs2[:elems_per_fold]\n",
        "fold1_t3 = timeseqs3[:elems_per_fold]\n",
        "fold1_t4 = timeseqs4[:elems_per_fold]\n",
        "with open('/content/drive/MyDrive/content/code/output_files/folds/fold1.csv', 'wb') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    for row, timeseq in izip(fold1, fold1_t):\n",
        "        spamwriter.writerow([unicode(s).encode(\"utf-8\") +'#{}'.format(t) for s, t in izip(row, timeseq)])\n",
        " \n",
        "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t3 = timeseqs3[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t4 = timeseqs4[elems_per_fold:2*elems_per_fold]\n",
        "with open('/content/drive/MyDrive/content/code/output_files/folds/fold2.csv', 'wb') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    for row, timeseq in izip(fold2, fold2_t):\n",
        "        spamwriter.writerow([unicode(s).encode(\"utf-8\") +'#{}'.format(t) for s, t in izip(row, timeseq)])\n",
        " \n",
        "fold3 = lines[2*elems_per_fold:]\n",
        "fold3_t = timeseqs[2*elems_per_fold:]\n",
        "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
        "fold3_t3 = timeseqs3[2*elems_per_fold:]\n",
        "fold3_t4 = timeseqs4[2*elems_per_fold:]\n",
        "with open('/content/drive/MyDrive/content/code/output_files/folds/fold3.csv', 'wb') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    for row, timeseq in izip(fold3, fold3_t):\n",
        "        spamwriter.writerow([unicode(s).encode(\"utf-8\") +'#{}'.format(t) for s, t in izip(row, timeseq)])\n",
        " \n",
        "lines = fold1 + fold2\n",
        "lines_t = fold1_t + fold2_t\n",
        "lines_t2 = fold1_t2 + fold2_t2\n",
        "lines_t3 = fold1_t3 + fold2_t3\n",
        "lines_t4 = fold1_t4 + fold2_t4\n",
        " \n",
        "\n",
        "step = 1\n",
        "sentences = []\n",
        "softness = 0\n",
        "next_chars = []\n",
        "lines = map(lambda x: x+'!',lines)\n",
        " \n",
        "sentences_t = []\n",
        "sentences_t2 = []\n",
        "sentences_t3 = []\n",
        "sentences_t4 = []\n",
        "next_chars_t = []\n",
        "next_chars_t2 = []\n",
        "next_chars_t3 = []\n",
        "next_chars_t4 = []\n",
        "for line, line_t, line_t2, line_t3, line_t4 in izip(lines, lines_t, lines_t2, lines_t3, lines_t4):\n",
        "    for i in range(0, len(line), step):\n",
        "        if i==0:\n",
        "            continue\n",
        " \n",
        "        #we add iteratively, first symbol of the line, then two first, three...\n",
        " \n",
        "        sentences.append(line[0: i])\n",
        "        sentences_t.append(line_t[0:i])\n",
        "        sentences_t2.append(line_t2[0:i])\n",
        "        sentences_t3.append(line_t3[0:i])\n",
        "        sentences_t4.append(line_t4[0:i])\n",
        "        next_chars.append(line[i])\n",
        "        if i==len(line)-1: # special case to deal time of end character\n",
        "            next_chars_t.append(0)\n",
        "            next_chars_t2.append(0)\n",
        "            next_chars_t3.append(0)\n",
        "            next_chars_t4.append(0)\n",
        "        else:\n",
        "            next_chars_t.append(line_t[i])\n",
        "            next_chars_t2.append(line_t2[i])\n",
        "            next_chars_t3.append(line_t3[i])\n",
        "            next_chars_t4.append(line_t4[i])\n",
        "print('nb sequences:', len(sentences))\n",
        " \n",
        "print('Vectorization...')\n",
        "num_features = len(chars)+5\n",
        "print('num features: {}'.format(num_features))\n",
        "X = np.zeros((len(sentences), maxlen, num_features), dtype=np.float32)\n",
        "y_a = np.zeros((len(sentences), len(target_chars)), dtype=np.float32)\n",
        "y_t = np.zeros((len(sentences)), dtype=np.float32)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    leftpad = maxlen-len(sentence)\n",
        "    next_t = next_chars_t[i]\n",
        "    sentence_t = sentences_t[i]\n",
        "    sentence_t2 = sentences_t2[i]\n",
        "    sentence_t3 = sentences_t3[i]\n",
        "    sentence_t4 = sentences_t4[i]\n",
        "    for t, char in enumerate(sentence):\n",
        "        multiset_abstraction = Counter(sentence[:t+1])\n",
        "        for c in chars:\n",
        "            if c==char: #this will encode present events to the right places\n",
        "                X[i, t+leftpad, char_indices[c]] = 1\n",
        "        X[i, t+leftpad, len(chars)] = t+1\n",
        "        X[i, t+leftpad, len(chars)+1] = sentence_t[t]/divisor\n",
        "        X[i, t+leftpad, len(chars)+2] = sentence_t2[t]/divisor2\n",
        "        X[i, t+leftpad, len(chars)+3] = sentence_t3[t]/86400\n",
        "        X[i, t+leftpad, len(chars)+4] = sentence_t4[t]/7\n",
        "    for c in target_chars:\n",
        "        if c==next_chars[i]:\n",
        "            y_a[i, target_char_indices[c]] = 1-softness\n",
        "        else:\n",
        "            y_a[i, target_char_indices[c]] = softness/(len(target_chars)-1)\n",
        "    y_t[i] = next_t/divisor\n",
        "    np.set_printoptions(threshold=sys.maxsize) #change np.nan to sys.maxsize\n",
        " \n",
        "# build the model: \n",
        "print('Build model...')\n",
        "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
        "# train a 2-layer LSTM with one shared layer\n",
        "l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input) # the shared layer. DROPOUT FROM 0.2 TO 0 AS NOT SUPPORTED ANYMORE\n",
        "b1 = BatchNormalization()(l1)\n",
        "l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in activity prediction. DROPOUT FROM 0.2 TO 0 AS NOT SUPPORTED ANYMORE\n",
        "b2_1 = BatchNormalization()(l2_1)\n",
        "l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in time prediction. DROPOUT FROM 0.2 TO 0 AS NOT SUPPORTED ANYMORE\n",
        "b2_2 = BatchNormalization()(l2_2)\n",
        "act_output = Dense(len(target_chars), activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)\n",
        "time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
        " \n",
        "model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
        " \n",
        "opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
        " \n",
        "model.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mae'}, optimizer=opt)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/content/code/output_files/models/model_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        " \n",
        "model.fit(X, {'act_output':y_a, 'time_output':y_t}, validation_split=0.2, verbose=2, callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, epochs=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unicodecsv\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-cp27-none-any.whl size=10768 sha256=0d3858c6046e21bed5ea0da8a06bee4be13d22afb52583f018e1bf89efdafdd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv\n",
            "Successfully installed unicodecsv-0.14.1\n",
            "first line excecuted\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "imports done\n",
            "divisor: 164.004640656\n",
            "divisor2: 6031.62953937\n",
            "total chars: 23, target chars: 24\n",
            "char_indices\n",
            "{u'\\xa3': 1, u'\\xa2': 0, u'\\xa5': 3, u'\\xa4': 2, u'\\xa7': 5, u'\\xa6': 4, u'\\xa9': 7, u'\\xa8': 6, u'\\xab': 9, u'\\xaa': 8, u'\\xad': 11, u'\\xac': 10, u'\\xaf': 13, u'\\xae': 12, u'\\xb1': 15, u'\\xb0': 14, u'\\xb3': 17, u'\\xb2': 16, u'\\xb5': 19, u'\\xb4': 18, u'\\xb7': 21, u'\\xb6': 20, u'\\xb8': 22}\n",
            "indices_char\n",
            "{0: u'\\xa2', 1: u'\\xa3', 2: u'\\xa4', 3: u'\\xa5', 4: u'\\xa6', 5: u'\\xa7', 6: u'\\xa8', 7: u'\\xa9', 8: u'\\xaa', 9: u'\\xab', 10: u'\\xac', 11: u'\\xad', 12: u'\\xae', 13: u'\\xaf', 14: u'\\xb0', 15: u'\\xb1', 16: u'\\xb2', 17: u'\\xb3', 18: u'\\xb4', 19: u'\\xb5', 20: u'\\xb6', 21: u'\\xb7', 22: u'\\xb8'}\n",
            "target_char_indices\n",
            "{u'!': 0, u'\\xa3': 2, u'\\xa2': 1, u'\\xa5': 4, u'\\xa4': 3, u'\\xa7': 6, u'\\xa6': 5, u'\\xa9': 8, u'\\xa8': 7, u'\\xab': 10, u'\\xaa': 9, u'\\xad': 12, u'\\xac': 11, u'\\xaf': 14, u'\\xae': 13, u'\\xb1': 16, u'\\xb0': 15, u'\\xb3': 18, u'\\xb2': 17, u'\\xb5': 20, u'\\xb4': 19, u'\\xb7': 22, u'\\xb6': 21, u'\\xb8': 23}\n",
            "target_indices_char\n",
            "{0: u'!', 1: u'\\xa2', 2: u'\\xa3', 3: u'\\xa4', 4: u'\\xa5', 5: u'\\xa6', 6: u'\\xa7', 7: u'\\xa8', 8: u'\\xa9', 9: u'\\xaa', 10: u'\\xab', 11: u'\\xac', 12: u'\\xad', 13: u'\\xae', 14: u'\\xaf', 15: u'\\xb0', 16: u'\\xb1', 17: u'\\xb2', 18: u'\\xb3', 19: u'\\xb4', 20: u'\\xb5', 21: u'\\xb6', 22: u'\\xb7', 23: u'\\xb8'}\n",
            "nb sequences: 109469\n",
            "Vectorization...\n",
            "num features: 28\n",
            "Build model...\n",
            "Train on 87575 samples, validate on 21894 samples\n",
            "Epoch 1/500\n",
            " - 256s - loss: 3.1014 - act_output_loss: 1.9946 - time_output_loss: 1.1065 - val_loss: 2.6311 - val_act_output_loss: 1.7066 - val_time_output_loss: 0.9217\n",
            "Epoch 2/500\n",
            " - 254s - loss: 2.6355 - act_output_loss: 1.6016 - time_output_loss: 1.0339 - val_loss: 2.3629 - val_act_output_loss: 1.4364 - val_time_output_loss: 0.9231\n",
            "Epoch 3/500\n",
            " - 254s - loss: 2.5310 - act_output_loss: 1.5050 - time_output_loss: 1.0259 - val_loss: 2.2831 - val_act_output_loss: 1.3659 - val_time_output_loss: 0.9133\n",
            "Epoch 4/500\n",
            " - 250s - loss: 2.4777 - act_output_loss: 1.4557 - time_output_loss: 1.0219 - val_loss: 2.2626 - val_act_output_loss: 1.3462 - val_time_output_loss: 0.9120\n",
            "Epoch 5/500\n",
            " - 249s - loss: 2.4404 - act_output_loss: 1.4212 - time_output_loss: 1.0193 - val_loss: 2.2254 - val_act_output_loss: 1.3020 - val_time_output_loss: 0.9190\n",
            "Epoch 6/500\n",
            " - 251s - loss: 2.4129 - act_output_loss: 1.3944 - time_output_loss: 1.0185 - val_loss: 2.2588 - val_act_output_loss: 1.3237 - val_time_output_loss: 0.9308\n",
            "Epoch 7/500\n",
            " - 250s - loss: 2.3939 - act_output_loss: 1.3774 - time_output_loss: 1.0166 - val_loss: 2.1850 - val_act_output_loss: 1.2748 - val_time_output_loss: 0.9059\n",
            "Epoch 8/500\n",
            " - 253s - loss: 2.3593 - act_output_loss: 1.3528 - time_output_loss: 1.0064 - val_loss: 2.1682 - val_act_output_loss: 1.2607 - val_time_output_loss: 0.9035\n",
            "Epoch 9/500\n",
            " - 252s - loss: 2.3456 - act_output_loss: 1.3396 - time_output_loss: 1.0060 - val_loss: 2.1644 - val_act_output_loss: 1.2568 - val_time_output_loss: 0.9035\n",
            "Epoch 10/500\n",
            " - 251s - loss: 2.3360 - act_output_loss: 1.3302 - time_output_loss: 1.0057 - val_loss: 2.1524 - val_act_output_loss: 1.2450 - val_time_output_loss: 0.9033\n",
            "Epoch 11/500\n",
            " - 254s - loss: 2.3260 - act_output_loss: 1.3210 - time_output_loss: 1.0049 - val_loss: 2.1522 - val_act_output_loss: 1.2467 - val_time_output_loss: 0.9016\n",
            "Epoch 12/500\n",
            " - 255s - loss: 2.3242 - act_output_loss: 1.3192 - time_output_loss: 1.0051 - val_loss: 2.1475 - val_act_output_loss: 1.2402 - val_time_output_loss: 0.9032\n",
            "Epoch 13/500\n",
            " - 254s - loss: 2.3148 - act_output_loss: 1.3098 - time_output_loss: 1.0050 - val_loss: 2.1420 - val_act_output_loss: 1.2367 - val_time_output_loss: 0.9010\n",
            "Epoch 14/500\n",
            " - 253s - loss: 2.3024 - act_output_loss: 1.2979 - time_output_loss: 1.0047 - val_loss: 2.1354 - val_act_output_loss: 1.2284 - val_time_output_loss: 0.9029\n",
            "Epoch 15/500\n",
            " - 253s - loss: 2.2987 - act_output_loss: 1.2944 - time_output_loss: 1.0041 - val_loss: 2.1337 - val_act_output_loss: 1.2261 - val_time_output_loss: 0.9035\n",
            "Epoch 16/500\n",
            " - 251s - loss: 2.3068 - act_output_loss: 1.3027 - time_output_loss: 1.0041 - val_loss: 2.1344 - val_act_output_loss: 1.2289 - val_time_output_loss: 0.9013\n",
            "Epoch 17/500\n",
            " - 251s - loss: 2.2893 - act_output_loss: 1.2852 - time_output_loss: 1.0040 - val_loss: 2.1496 - val_act_output_loss: 1.2443 - val_time_output_loss: 0.9011\n",
            "Epoch 18/500\n",
            " - 251s - loss: 2.2921 - act_output_loss: 1.2883 - time_output_loss: 1.0038 - val_loss: 2.1276 - val_act_output_loss: 1.2226 - val_time_output_loss: 0.9007\n",
            "Epoch 19/500\n",
            " - 251s - loss: 2.2827 - act_output_loss: 1.2790 - time_output_loss: 1.0037 - val_loss: 2.1168 - val_act_output_loss: 1.2128 - val_time_output_loss: 0.8998\n",
            "Epoch 20/500\n",
            " - 250s - loss: 2.2756 - act_output_loss: 1.2720 - time_output_loss: 1.0035 - val_loss: 2.1201 - val_act_output_loss: 1.2157 - val_time_output_loss: 0.9004\n",
            "Epoch 21/500\n",
            " - 249s - loss: 2.2724 - act_output_loss: 1.2689 - time_output_loss: 1.0035 - val_loss: 2.1224 - val_act_output_loss: 1.2184 - val_time_output_loss: 0.9000\n",
            "Epoch 22/500\n",
            " - 250s - loss: 2.2666 - act_output_loss: 1.2633 - time_output_loss: 1.0030 - val_loss: 2.1137 - val_act_output_loss: 1.2101 - val_time_output_loss: 0.8995\n",
            "Epoch 23/500\n",
            " - 249s - loss: 2.2645 - act_output_loss: 1.2612 - time_output_loss: 1.0033 - val_loss: 2.1156 - val_act_output_loss: 1.2104 - val_time_output_loss: 0.9010\n",
            "Epoch 24/500\n",
            " - 250s - loss: 2.2691 - act_output_loss: 1.2660 - time_output_loss: 1.0032 - val_loss: 2.1155 - val_act_output_loss: 1.2112 - val_time_output_loss: 0.9001\n",
            "Epoch 25/500\n",
            " - 249s - loss: 2.2607 - act_output_loss: 1.2576 - time_output_loss: 1.0034 - val_loss: 2.1170 - val_act_output_loss: 1.2124 - val_time_output_loss: 0.9002\n",
            "Epoch 26/500\n",
            " - 249s - loss: 2.2541 - act_output_loss: 1.2512 - time_output_loss: 1.0032 - val_loss: 2.1090 - val_act_output_loss: 1.2038 - val_time_output_loss: 0.9014\n",
            "Epoch 27/500\n",
            " - 249s - loss: 2.2505 - act_output_loss: 1.2478 - time_output_loss: 1.0027 - val_loss: 2.1078 - val_act_output_loss: 1.2044 - val_time_output_loss: 0.8994\n",
            "Epoch 28/500\n",
            " - 248s - loss: 2.2512 - act_output_loss: 1.2486 - time_output_loss: 1.0025 - val_loss: 2.1134 - val_act_output_loss: 1.2083 - val_time_output_loss: 0.9010\n",
            "Epoch 29/500\n",
            " - 248s - loss: 2.2478 - act_output_loss: 1.2452 - time_output_loss: 1.0026 - val_loss: 2.1142 - val_act_output_loss: 1.2094 - val_time_output_loss: 0.9005\n",
            "Epoch 30/500\n",
            " - 247s - loss: 2.2430 - act_output_loss: 1.2402 - time_output_loss: 1.0029 - val_loss: 2.1091 - val_act_output_loss: 1.2051 - val_time_output_loss: 0.8999\n",
            "Epoch 31/500\n",
            " - 245s - loss: 2.2409 - act_output_loss: 1.2383 - time_output_loss: 1.0030 - val_loss: 2.1195 - val_act_output_loss: 1.2154 - val_time_output_loss: 0.8999\n",
            "Epoch 32/500\n",
            " - 245s - loss: 2.2366 - act_output_loss: 1.2340 - time_output_loss: 1.0026 - val_loss: 2.1073 - val_act_output_loss: 1.2028 - val_time_output_loss: 0.9006\n",
            "Epoch 33/500\n",
            " - 245s - loss: 2.2375 - act_output_loss: 1.2351 - time_output_loss: 1.0024 - val_loss: 2.1054 - val_act_output_loss: 1.2019 - val_time_output_loss: 0.8994\n",
            "Epoch 34/500\n",
            " - 244s - loss: 2.2337 - act_output_loss: 1.2313 - time_output_loss: 1.0024 - val_loss: 2.1086 - val_act_output_loss: 1.2055 - val_time_output_loss: 0.8990\n",
            "Epoch 35/500\n",
            " - 243s - loss: 2.2333 - act_output_loss: 1.2309 - time_output_loss: 1.0023 - val_loss: 2.1044 - val_act_output_loss: 1.2005 - val_time_output_loss: 0.8997\n",
            "Epoch 36/500\n",
            " - 243s - loss: 2.2237 - act_output_loss: 1.2216 - time_output_loss: 1.0019 - val_loss: 2.1052 - val_act_output_loss: 1.2015 - val_time_output_loss: 0.8994\n",
            "Epoch 37/500\n",
            " - 243s - loss: 2.2256 - act_output_loss: 1.2235 - time_output_loss: 1.0022 - val_loss: 2.1045 - val_act_output_loss: 1.2006 - val_time_output_loss: 0.8997\n",
            "Epoch 38/500\n",
            " - 243s - loss: 2.2222 - act_output_loss: 1.2201 - time_output_loss: 1.0021 - val_loss: 2.1053 - val_act_output_loss: 1.2004 - val_time_output_loss: 0.9007\n",
            "Epoch 39/500\n",
            " - 244s - loss: 2.2219 - act_output_loss: 1.2199 - time_output_loss: 1.0023 - val_loss: 2.0979 - val_act_output_loss: 1.1945 - val_time_output_loss: 0.8994\n",
            "Epoch 40/500\n",
            " - 243s - loss: 2.2163 - act_output_loss: 1.2144 - time_output_loss: 1.0018 - val_loss: 2.0994 - val_act_output_loss: 1.1962 - val_time_output_loss: 0.8990\n",
            "Epoch 41/500\n",
            " - 242s - loss: 2.2174 - act_output_loss: 1.2154 - time_output_loss: 1.0019 - val_loss: 2.0988 - val_act_output_loss: 1.1949 - val_time_output_loss: 0.8999\n",
            "Epoch 42/500\n",
            " - 243s - loss: 2.2146 - act_output_loss: 1.2127 - time_output_loss: 1.0017 - val_loss: 2.0980 - val_act_output_loss: 1.1945 - val_time_output_loss: 0.8994\n",
            "Epoch 43/500\n",
            " - 244s - loss: 2.2072 - act_output_loss: 1.2054 - time_output_loss: 1.0017 - val_loss: 2.0987 - val_act_output_loss: 1.1944 - val_time_output_loss: 0.9002\n",
            "Epoch 44/500\n",
            " - 244s - loss: 2.2079 - act_output_loss: 1.2061 - time_output_loss: 1.0016 - val_loss: 2.0994 - val_act_output_loss: 1.1964 - val_time_output_loss: 0.8989\n",
            "Epoch 45/500\n",
            " - 243s - loss: 2.2086 - act_output_loss: 1.2069 - time_output_loss: 1.0018 - val_loss: 2.1022 - val_act_output_loss: 1.1993 - val_time_output_loss: 0.8988\n",
            "Epoch 46/500\n",
            " - 244s - loss: 2.2131 - act_output_loss: 1.2112 - time_output_loss: 1.0018 - val_loss: 2.1010 - val_act_output_loss: 1.1969 - val_time_output_loss: 0.8999\n",
            "Epoch 47/500\n",
            " - 248s - loss: 2.2014 - act_output_loss: 1.1990 - time_output_loss: 1.0026 - val_loss: 2.1009 - val_act_output_loss: 1.1980 - val_time_output_loss: 0.8988\n",
            "Epoch 48/500\n",
            " - 246s - loss: 2.2014 - act_output_loss: 1.1996 - time_output_loss: 1.0018 - val_loss: 2.1009 - val_act_output_loss: 1.1975 - val_time_output_loss: 0.8995\n",
            "Epoch 49/500\n",
            " - 244s - loss: 2.1979 - act_output_loss: 1.1965 - time_output_loss: 1.0014 - val_loss: 2.1002 - val_act_output_loss: 1.1968 - val_time_output_loss: 0.8993\n",
            "Epoch 50/500\n",
            " - 243s - loss: 2.1797 - act_output_loss: 1.1786 - time_output_loss: 1.0015 - val_loss: 2.0897 - val_act_output_loss: 1.1875 - val_time_output_loss: 0.8983\n",
            "Epoch 51/500\n",
            " - 244s - loss: 2.1750 - act_output_loss: 1.1743 - time_output_loss: 1.0006 - val_loss: 2.0928 - val_act_output_loss: 1.1901 - val_time_output_loss: 0.8986\n",
            "Epoch 52/500\n",
            " - 247s - loss: 2.1738 - act_output_loss: 1.1731 - time_output_loss: 1.0006 - val_loss: 2.0907 - val_act_output_loss: 1.1886 - val_time_output_loss: 0.8982\n",
            "Epoch 53/500\n",
            " - 245s - loss: 2.1722 - act_output_loss: 1.1716 - time_output_loss: 1.0007 - val_loss: 2.0964 - val_act_output_loss: 1.1930 - val_time_output_loss: 0.8995\n",
            "Epoch 54/500\n",
            " - 245s - loss: 2.1683 - act_output_loss: 1.1677 - time_output_loss: 1.0004 - val_loss: 2.0949 - val_act_output_loss: 1.1921 - val_time_output_loss: 0.8987\n",
            "Epoch 55/500\n",
            " - 241s - loss: 2.1700 - act_output_loss: 1.1694 - time_output_loss: 1.0006 - val_loss: 2.0932 - val_act_output_loss: 1.1907 - val_time_output_loss: 0.8984\n",
            "Epoch 56/500\n",
            " - 241s - loss: 2.1677 - act_output_loss: 1.1669 - time_output_loss: 1.0006 - val_loss: 2.0912 - val_act_output_loss: 1.1885 - val_time_output_loss: 0.8987\n",
            "Epoch 57/500\n",
            " - 243s - loss: 2.1686 - act_output_loss: 1.1684 - time_output_loss: 1.0004 - val_loss: 2.0900 - val_act_output_loss: 1.1876 - val_time_output_loss: 0.8982\n",
            "Epoch 58/500\n",
            " - 263s - loss: 2.1620 - act_output_loss: 1.1615 - time_output_loss: 1.0004 - val_loss: 2.0898 - val_act_output_loss: 1.1875 - val_time_output_loss: 0.8983\n",
            "Epoch 59/500\n",
            " - 265s - loss: 2.1632 - act_output_loss: 1.1629 - time_output_loss: 1.0002 - val_loss: 2.0949 - val_act_output_loss: 1.1926 - val_time_output_loss: 0.8986\n",
            "Epoch 60/500\n",
            " - 260s - loss: 2.1655 - act_output_loss: 1.1648 - time_output_loss: 1.0008 - val_loss: 2.0911 - val_act_output_loss: 1.1888 - val_time_output_loss: 0.8983\n",
            "Epoch 61/500\n",
            " - 261s - loss: 2.1550 - act_output_loss: 1.1549 - time_output_loss: 1.0001 - val_loss: 2.0875 - val_act_output_loss: 1.1854 - val_time_output_loss: 0.8981\n",
            "Epoch 62/500\n",
            " - 266s - loss: 2.1499 - act_output_loss: 1.1500 - time_output_loss: 1.0002 - val_loss: 2.0879 - val_act_output_loss: 1.1857 - val_time_output_loss: 0.8983\n",
            "Epoch 63/500\n",
            " - 267s - loss: 2.1507 - act_output_loss: 1.1508 - time_output_loss: 0.9998 - val_loss: 2.0884 - val_act_output_loss: 1.1864 - val_time_output_loss: 0.8979\n",
            "Epoch 64/500\n",
            " - 267s - loss: 2.1434 - act_output_loss: 1.1435 - time_output_loss: 0.9999 - val_loss: 2.0893 - val_act_output_loss: 1.1872 - val_time_output_loss: 0.8980\n",
            "Epoch 65/500\n",
            " - 268s - loss: 2.1477 - act_output_loss: 1.1479 - time_output_loss: 0.9997 - val_loss: 2.0902 - val_act_output_loss: 1.1882 - val_time_output_loss: 0.8979\n",
            "Epoch 66/500\n",
            " - 266s - loss: 2.1462 - act_output_loss: 1.1465 - time_output_loss: 0.9998 - val_loss: 2.0921 - val_act_output_loss: 1.1899 - val_time_output_loss: 0.8982\n",
            "Epoch 67/500\n",
            " - 262s - loss: 2.1450 - act_output_loss: 1.1451 - time_output_loss: 0.9997 - val_loss: 2.0910 - val_act_output_loss: 1.1892 - val_time_output_loss: 0.8978\n",
            "Epoch 68/500\n",
            " - 262s - loss: 2.1435 - act_output_loss: 1.1439 - time_output_loss: 0.9999 - val_loss: 2.0901 - val_act_output_loss: 1.1879 - val_time_output_loss: 0.8981\n",
            "Epoch 69/500\n",
            " - 261s - loss: 2.1425 - act_output_loss: 1.1427 - time_output_loss: 0.9998 - val_loss: 2.0914 - val_act_output_loss: 1.1896 - val_time_output_loss: 0.8978\n",
            "Epoch 70/500\n",
            " - 259s - loss: 2.1415 - act_output_loss: 1.1420 - time_output_loss: 0.9993 - val_loss: 2.0891 - val_act_output_loss: 1.1874 - val_time_output_loss: 0.8977\n",
            "Epoch 71/500\n",
            " - 262s - loss: 2.1399 - act_output_loss: 1.1404 - time_output_loss: 0.9998 - val_loss: 2.0915 - val_act_output_loss: 1.1896 - val_time_output_loss: 0.8980\n",
            "Epoch 72/500\n",
            " - 258s - loss: 2.1350 - act_output_loss: 1.1356 - time_output_loss: 0.9996 - val_loss: 2.0889 - val_act_output_loss: 1.1873 - val_time_output_loss: 0.8977\n",
            "Epoch 73/500\n",
            " - 245s - loss: 2.1353 - act_output_loss: 1.1361 - time_output_loss: 0.9992 - val_loss: 2.0894 - val_act_output_loss: 1.1876 - val_time_output_loss: 0.8978\n",
            "Epoch 74/500\n",
            " - 242s - loss: 2.1339 - act_output_loss: 1.1347 - time_output_loss: 0.9991 - val_loss: 2.0888 - val_act_output_loss: 1.1874 - val_time_output_loss: 0.8975\n",
            "Epoch 75/500\n",
            " - 244s - loss: 2.1336 - act_output_loss: 1.1342 - time_output_loss: 0.9995 - val_loss: 2.0876 - val_act_output_loss: 1.1861 - val_time_output_loss: 0.8975\n",
            "Epoch 76/500\n",
            " - 246s - loss: 2.1321 - act_output_loss: 1.1327 - time_output_loss: 0.9991 - val_loss: 2.0906 - val_act_output_loss: 1.1893 - val_time_output_loss: 0.8975\n",
            "Epoch 77/500\n",
            " - 245s - loss: 2.1310 - act_output_loss: 1.1317 - time_output_loss: 0.9993 - val_loss: 2.0890 - val_act_output_loss: 1.1875 - val_time_output_loss: 0.8976\n",
            "Epoch 78/500\n",
            " - 243s - loss: 2.1302 - act_output_loss: 1.1308 - time_output_loss: 0.9994 - val_loss: 2.0904 - val_act_output_loss: 1.1888 - val_time_output_loss: 0.8975\n",
            "Epoch 79/500\n",
            " - 242s - loss: 2.1319 - act_output_loss: 1.1328 - time_output_loss: 0.9991 - val_loss: 2.0908 - val_act_output_loss: 1.1893 - val_time_output_loss: 0.8976\n",
            "Epoch 80/500\n",
            " - 241s - loss: 2.1261 - act_output_loss: 1.1269 - time_output_loss: 0.9991 - val_loss: 2.0921 - val_act_output_loss: 1.1903 - val_time_output_loss: 0.8979\n",
            "Epoch 81/500\n",
            " - 241s - loss: 2.1284 - act_output_loss: 1.1293 - time_output_loss: 0.9990 - val_loss: 2.0917 - val_act_output_loss: 1.1902 - val_time_output_loss: 0.8975\n",
            "Epoch 82/500\n",
            " - 241s - loss: 2.1282 - act_output_loss: 1.1293 - time_output_loss: 0.9990 - val_loss: 2.0899 - val_act_output_loss: 1.1886 - val_time_output_loss: 0.8974\n",
            "Epoch 83/500\n",
            " - 241s - loss: 2.1265 - act_output_loss: 1.1275 - time_output_loss: 0.9993 - val_loss: 2.0903 - val_act_output_loss: 1.1889 - val_time_output_loss: 0.8975\n",
            "Epoch 84/500\n",
            " - 241s - loss: 2.1236 - act_output_loss: 1.1244 - time_output_loss: 0.9991 - val_loss: 2.0898 - val_act_output_loss: 1.1884 - val_time_output_loss: 0.8974\n",
            "Epoch 85/500\n",
            " - 242s - loss: 2.1238 - act_output_loss: 1.1246 - time_output_loss: 0.9996 - val_loss: 2.0899 - val_act_output_loss: 1.1886 - val_time_output_loss: 0.8974\n",
            "Epoch 86/500\n",
            " - 241s - loss: 2.1258 - act_output_loss: 1.1264 - time_output_loss: 0.9993 - val_loss: 2.0912 - val_act_output_loss: 1.1899 - val_time_output_loss: 0.8974\n",
            "Epoch 87/500\n",
            " - 242s - loss: 2.1227 - act_output_loss: 1.1236 - time_output_loss: 0.9990 - val_loss: 2.0901 - val_act_output_loss: 1.1888 - val_time_output_loss: 0.8974\n",
            "Epoch 88/500\n",
            " - 241s - loss: 2.1247 - act_output_loss: 1.1258 - time_output_loss: 0.9992 - val_loss: 2.0909 - val_act_output_loss: 1.1896 - val_time_output_loss: 0.8974\n",
            "Epoch 89/500\n",
            " - 241s - loss: 2.1197 - act_output_loss: 1.1209 - time_output_loss: 0.9988 - val_loss: 2.0910 - val_act_output_loss: 1.1897 - val_time_output_loss: 0.8973\n",
            "Epoch 90/500\n",
            " - 245s - loss: 2.1227 - act_output_loss: 1.1239 - time_output_loss: 0.9988 - val_loss: 2.0915 - val_act_output_loss: 1.1903 - val_time_output_loss: 0.8973\n",
            "Epoch 91/500\n",
            " - 246s - loss: 2.1240 - act_output_loss: 1.1251 - time_output_loss: 0.9988 - val_loss: 2.0902 - val_act_output_loss: 1.1890 - val_time_output_loss: 0.8973\n",
            "Epoch 92/500\n",
            " - 242s - loss: 2.1261 - act_output_loss: 1.1269 - time_output_loss: 0.9991 - val_loss: 2.0902 - val_act_output_loss: 1.1890 - val_time_output_loss: 0.8973\n",
            "Epoch 93/500\n",
            " - 242s - loss: 2.1266 - act_output_loss: 1.1276 - time_output_loss: 0.9989 - val_loss: 2.0906 - val_act_output_loss: 1.1895 - val_time_output_loss: 0.8972\n",
            "Epoch 94/500\n",
            " - 242s - loss: 2.1215 - act_output_loss: 1.1225 - time_output_loss: 0.9990 - val_loss: 2.0902 - val_act_output_loss: 1.1890 - val_time_output_loss: 0.8973\n",
            "Epoch 95/500\n",
            " - 241s - loss: 2.1243 - act_output_loss: 1.1256 - time_output_loss: 0.9987 - val_loss: 2.0899 - val_act_output_loss: 1.1888 - val_time_output_loss: 0.8972\n",
            "Epoch 96/500\n",
            " - 243s - loss: 2.1216 - act_output_loss: 1.1231 - time_output_loss: 0.9988 - val_loss: 2.0904 - val_act_output_loss: 1.1892 - val_time_output_loss: 0.8973\n",
            "Epoch 97/500\n",
            " - 240s - loss: 2.1199 - act_output_loss: 1.1211 - time_output_loss: 0.9987 - val_loss: 2.0903 - val_act_output_loss: 1.1891 - val_time_output_loss: 0.8973\n",
            "Epoch 98/500\n",
            " - 240s - loss: 2.1194 - act_output_loss: 1.1209 - time_output_loss: 0.9985 - val_loss: 2.0909 - val_act_output_loss: 1.1896 - val_time_output_loss: 0.8973\n",
            "Epoch 99/500\n",
            " - 240s - loss: 2.1197 - act_output_loss: 1.1207 - time_output_loss: 0.9990 - val_loss: 2.0907 - val_act_output_loss: 1.1895 - val_time_output_loss: 0.8972\n",
            "Epoch 100/500\n",
            " - 243s - loss: 2.1230 - act_output_loss: 1.1242 - time_output_loss: 0.9987 - val_loss: 2.0904 - val_act_output_loss: 1.1892 - val_time_output_loss: 0.8973\n",
            "Epoch 101/500\n",
            " - 256s - loss: 2.1202 - act_output_loss: 1.1216 - time_output_loss: 0.9986 - val_loss: 2.0909 - val_act_output_loss: 1.1896 - val_time_output_loss: 0.8973\n",
            "Epoch 102/500\n",
            " - 262s - loss: 2.1165 - act_output_loss: 1.1176 - time_output_loss: 0.9989 - val_loss: 2.0910 - val_act_output_loss: 1.1897 - val_time_output_loss: 0.8974\n",
            "Epoch 103/500\n",
            " - 261s - loss: 2.1217 - act_output_loss: 1.1232 - time_output_loss: 0.9986 - val_loss: 2.0907 - val_act_output_loss: 1.1894 - val_time_output_loss: 0.8974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7ff82aeffe10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2y-YJcCmJ4a"
      },
      "source": [
        "##**Code: evaluate suffix and remaining time**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTFvgWlnsE6Y"
      },
      "source": [
        "<font color='orange'>- In order to access the latest model constructed, had to type the last model from the output_files --> models map, would be handy that the code can find this latest model automatically with for example code like: 'model_{epoch:02d}-{val_loss:.2f}.h5' </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfhKIEbpmMHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e40291-7ca5-48e2-9f4d-55f26dd003a1"
      },
      "source": [
        "'''\n",
        "this script takes as input the LSTM or RNN weights found by train.py\n",
        "change the path in line 178 (187 beter) of this script to point to the h5 file\n",
        "with LSTM or RNN weights generated by train.py\n",
        "\n",
        "Author: Niek Tax\n",
        "'''\n",
        "!pip install distance\n",
        "!pip install jellyfish\n",
        "!pip install unicodecsv\n",
        "!pip install keras.models\n",
        "!pip install np_utils\n",
        "\n",
        "from __future__ import division\n",
        "from keras.models import load_model\n",
        "import csv\n",
        "import copy\n",
        "import numpy as np\n",
        "import distance\n",
        "from itertools import izip\n",
        "from jellyfish._jellyfish import damerau_levenshtein_distance\n",
        "import unicodecsv\n",
        "from sklearn import metrics\n",
        "from math import sqrt\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "eventlog = \"edxpddf.csv\"\n",
        "csvfile = open('/content/drive/MyDrive/content/data/%s' % eventlog, 'r')\n",
        "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "next(spamreader, None)  # skip the headers\n",
        "ascii_offset = 161\n",
        "\n",
        "lastcase = ''\n",
        "line = ''\n",
        "firstLine = True\n",
        "lines = []\n",
        "caseids = []\n",
        "timeseqs = []\n",
        "timeseqs2 = []\n",
        "times = []\n",
        "times2 = []\n",
        "numlines = 0\n",
        "casestarttime = None\n",
        "lasteventtime = None\n",
        "for row in spamreader:\n",
        "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
        "    if row[0]!=lastcase:\n",
        "        caseids.append(row[0])\n",
        "        casestarttime = t\n",
        "        lasteventtime = t\n",
        "        lastcase = row[0]\n",
        "        if not firstLine:        \n",
        "            lines.append(line)\n",
        "            timeseqs.append(times)\n",
        "            timeseqs2.append(times2)\n",
        "        line = ''\n",
        "        times = []\n",
        "        times2 = []\n",
        "        numlines+=1\n",
        "    line+=unichr(int(row[1])+ascii_offset)\n",
        "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
        "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
        "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
        "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
        "    times.append(timediff)\n",
        "    times2.append(timediff2)\n",
        "    lasteventtime = t\n",
        "    firstLine = False\n",
        "\n",
        "# add last case\n",
        "lines.append(line)\n",
        "timeseqs.append(times)\n",
        "timeseqs2.append(times2)\n",
        "numlines+=1\n",
        "\n",
        "divisor = np.mean([item for sublist in timeseqs for item in sublist])\n",
        "print('divisor: {}'.format(divisor))\n",
        "divisor2 = np.mean([item for sublist in timeseqs2 for item in sublist])\n",
        "print('divisor2: {}'.format(divisor2))\n",
        "divisor3 = np.mean(map(lambda x: np.mean(map(lambda y: x[len(x)-1]-y, x)), timeseqs2))\n",
        "print('divisor3: {}'.format(divisor3))\n",
        "\n",
        "elems_per_fold = int(round(numlines/3))\n",
        "fold1 = lines[:elems_per_fold]\n",
        "fold1_c = caseids[:elems_per_fold]\n",
        "fold1_t = timeseqs[:elems_per_fold]\n",
        "fold1_t2 = timeseqs2[:elems_per_fold]\n",
        "\n",
        "print(\"elems_per_fold: \")\n",
        "print(elems_per_fold)\n",
        "\n",
        "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
        "fold2_c = caseids[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
        "\n",
        "lines = fold1 + fold2\n",
        "caseids = fold1_c + fold2_c\n",
        "lines_t = fold1_t + fold2_t\n",
        "lines_t2 = fold1_t2 + fold2_t2\n",
        "\n",
        "step = 1\n",
        "sentences = []\n",
        "softness = 0\n",
        "next_chars = []\n",
        "lines = map(lambda x: x+'!',lines)\n",
        "maxlen = max(map(lambda x: len(x),lines))\n",
        "\n",
        "chars = map(lambda x : set(x),lines)\n",
        "chars = list(set().union(*chars))\n",
        "chars.sort()\n",
        "target_chars = copy.copy(chars)\n",
        "chars.remove('!')\n",
        "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
        "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
        "print(\"indices_char\")\n",
        "print(indices_char)\n",
        "print(\"target_indices_char\")\n",
        "print(target_indices_char)\n",
        "\n",
        "\n",
        "lastcase = ''\n",
        "line = ''\n",
        "firstLine = True\n",
        "lines = []\n",
        "caseids = []\n",
        "timeseqs = []  # relative time since previous event\n",
        "timeseqs2 = [] # relative time since case start\n",
        "timeseqs3 = [] # absolute time of previous event\n",
        "times = []\n",
        "times2 = []\n",
        "times3 = []\n",
        "numlines = 0\n",
        "casestarttime = None\n",
        "lasteventtime = None\n",
        "csvfile = open('/content/drive/MyDrive/content/data/%s' % eventlog, 'r')\n",
        "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "next(spamreader, None)  # skip the headers\n",
        "for row in spamreader:\n",
        "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
        "    if row[0]!=lastcase:\n",
        "        caseids.append(row[0])\n",
        "        casestarttime = t\n",
        "        lasteventtime = t\n",
        "        lastcase = row[0]\n",
        "        if not firstLine:        \n",
        "            lines.append(line)\n",
        "            timeseqs.append(times)\n",
        "            timeseqs2.append(times2)\n",
        "            timeseqs3.append(times3)\n",
        "        line = ''\n",
        "        times = []\n",
        "        times2 = []\n",
        "        times3 = []\n",
        "        numlines+=1\n",
        "    line+=unichr(int(row[1])+ascii_offset)\n",
        "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
        "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
        "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
        "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
        "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
        "    times.append(timediff)\n",
        "    times2.append(timediff2)\n",
        "    times3.append(datetime.fromtimestamp(time.mktime(t)))\n",
        "    lasteventtime = t\n",
        "    firstLine = False\n",
        "\n",
        "# add last case\n",
        "lines.append(line)\n",
        "timeseqs.append(times)\n",
        "timeseqs2.append(times2)\n",
        "timeseqs3.append(times3)\n",
        "numlines+=1\n",
        "\n",
        "fold3 = lines[2*elems_per_fold:]\n",
        "fold3_c = caseids[2*elems_per_fold:]\n",
        "fold3_t = timeseqs[2*elems_per_fold:]\n",
        "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
        "fold3_t3 = timeseqs3[2*elems_per_fold:]\n",
        "\n",
        "lines = fold3\n",
        "caseids = fold3_c\n",
        "lines_t = fold3_t\n",
        "lines_t2 = fold3_t2\n",
        "lines_t3 = fold3_t3\n",
        "\n",
        "# set parameters\n",
        "#predict_size = maxlen\n",
        "predict_size = 1\n",
        "\n",
        "# load model, set this to the model generated by train.py\n",
        "model = load_model('/content/drive/MyDrive/content/code/output_files/models/model_63-2.09.h5')\n",
        "\n",
        "# define helper functions\n",
        "def encode(sentence, times, times3, maxlen=maxlen):\n",
        "    num_features = len(chars)+5\n",
        "    X = np.zeros((1, maxlen, num_features), dtype=np.float32)\n",
        "    leftpad = maxlen-len(sentence)\n",
        "    times2 = np.cumsum(times)\n",
        "    for t, char in enumerate(sentence):\n",
        "        midnight = times3[t].replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "        timesincemidnight = times3[t]-midnight\n",
        "        multiset_abstraction = Counter(sentence[:t+1])\n",
        "        for c in chars:\n",
        "            if c==char:\n",
        "                X[0, t+leftpad, char_indices[c]] = 1\n",
        "        X[0, t+leftpad, len(chars)] = t+1\n",
        "        X[0, t+leftpad, len(chars)+1] = times[t]/divisor\n",
        "        X[0, t+leftpad, len(chars)+2] = times2[t]/divisor2\n",
        "        X[0, t+leftpad, len(chars)+3] = timesincemidnight.seconds/86400\n",
        "        X[0, t+leftpad, len(chars)+4] = times3[t].weekday()/7\n",
        "    return X\n",
        "\n",
        "def getSymbol(predictions):\n",
        "    maxPrediction = 0\n",
        "    symbol = ''\n",
        "    i = 0;\n",
        "    for prediction in predictions:\n",
        "        if(prediction>=maxPrediction):\n",
        "            maxPrediction = prediction\n",
        "            symbol = target_indices_char[i]\n",
        "        i += 1\n",
        "    return symbol\n",
        "\n",
        "one_ahead_gt = []\n",
        "one_ahead_pred = []\n",
        "\n",
        "two_ahead_gt = []\n",
        "two_ahead_pred = []\n",
        "\n",
        "three_ahead_gt = []\n",
        "three_ahead_pred = []\n",
        "\n",
        "detailedpredictions = pd.DataFrame()\n",
        "\n",
        "# make predictions\n",
        "with open('/content/drive/MyDrive/content/code/output_files/results/suffix_and_remaining_time_%s' % eventlog, mode='wb') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    spamwriter.writerow([\"CaseID\", \"Prefix length\", \"Groud truth\", \"Predicted\", \"Levenshtein\", \"Damerau\", \"Jaccard\", \"Ground truth times\", \"Predicted times\", \"RMSE\", \"MAE\"])\n",
        "    for prefix_size in range(2,maxlen):\n",
        "        print(prefix_size)\n",
        "        for line, caseid, times, times2, times3 in izip(lines, caseids, lines_t, lines_t2, lines_t3):\n",
        "            times.append(0)\n",
        "            cropped_line = ''.join(line[:prefix_size])\n",
        "            cropped_times = times[:prefix_size]\n",
        "            cropped_times3 = times3[:prefix_size]\n",
        "            if len(times2)<prefix_size:\n",
        "                continue # make no prediction for this case, since this case has ended already\n",
        "            ground_truth = ''.join(line[prefix_size:prefix_size+predict_size])\n",
        "            ground_truth_t = times2[prefix_size-1]\n",
        "            case_end_time = times2[len(times2)-1]\n",
        "            ground_truth_t = case_end_time-ground_truth_t\n",
        "            predicted = ''\n",
        "            total_predicted_time = 0\n",
        "            for i in range(predict_size):\n",
        "                enc = encode(cropped_line, cropped_times, cropped_times3)\n",
        "                y = model.predict(enc, verbose=0) # make predictions\n",
        "                # split predictions into seperate activity and time predictions\n",
        "                y_char = y[0][0] \n",
        "                y_t = y[1][0][0]\n",
        "                prediction = getSymbol(y_char) # undo one-hot encoding           \n",
        "                cropped_line += prediction\n",
        "                if y_t<0:\n",
        "                    y_t=0\n",
        "                cropped_times.append(y_t)\n",
        "                if prediction == '!': # end of case was just predicted, therefore, stop predicting further into the future\n",
        "                    one_ahead_pred.append(total_predicted_time)\n",
        "                    one_ahead_gt.append(ground_truth_t)\n",
        "                    print('! predicted, end case')\n",
        "                    break\n",
        "                y_t = y_t * divisor3\n",
        "                cropped_times3.append(cropped_times3[-1] + timedelta(seconds=y_t))\n",
        "                total_predicted_time = total_predicted_time + y_t\n",
        "                predicted += prediction\n",
        "            output = []\n",
        "            if len(ground_truth)>0:\n",
        "                output.append(caseid)\n",
        "                output.append(prefix_size)\n",
        "                output.append(unicode(ground_truth).encode(\"utf-8\"))\n",
        "                output.append(unicode(predicted).encode(\"utf-8\"))\n",
        "                detailedpredictions = detailedpredictions.append(pd.DataFrame(y_char, columns=range(1), index=range(24)).transpose())\n",
        "                output.append(1 - distance.nlevenshtein(predicted, ground_truth))\n",
        "                dls = 1 - (damerau_levenshtein_distance(unicode(predicted), unicode(ground_truth)) / max(len(predicted),len(ground_truth)))\n",
        "                if dls<0:\n",
        "                    dls=0 # we encountered problems with Damerau-Levenshtein Similarity on some linux machines where the default character encoding of the operating system caused it to be negative, this should never be the case\n",
        "                output.append(dls)\n",
        "                output.append(1 - distance.jaccard(predicted, ground_truth))\n",
        "                output.append(ground_truth_t)\n",
        "                output.append(total_predicted_time)\n",
        "                output.append('')\n",
        "                output.append(metrics.mean_absolute_error([ground_truth_t], [total_predicted_time]))\n",
        "                spamwriter.writerow(output)\n",
        "\n",
        "print(\"process finished\")                \n",
        "detailedpredictions.to_csv(\"/content/drive/MyDrive/content/code/output_files/results/detailedpredictions.csv\" , sep=',')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting distance\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n",
            "\r\u001b[K     |                              | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |                            | 20kB 10.2MB/s eta 0:00:01\r\u001b[K     |                          | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |                        | 40kB 7.3MB/s eta 0:00:01\r\u001b[K     |                       | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |                     | 61kB 4.6MB/s eta 0:00:01\r\u001b[K     |                   | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |                 | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |               | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |             | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |            | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |          | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |        | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |      | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |    | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |   | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     | | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     || 184kB 5.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-cp27-none-any.whl size=16262 sha256=8dcc751e0de31d68557cad4f800cab8b3c7d3a29064e8f7ec62d70072f96a91e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/aa/e1/dbba9e7b6d397d645d0f12db1c66dbae9c5442b39b001db18e\n",
            "Successfully built distance\n",
            "Installing collected packages: distance\n",
            "Successfully installed distance-0.1.3\n",
            "Collecting jellyfish\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/3f/60ac86fb43dfbf976768e80674b5538e535f6eca5aa7806cf2fdfd63550f/jellyfish-0.6.1.tar.gz (132kB)\n",
            "\u001b[K     || 133kB 5.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.6.1-cp27-cp27mu-linux_x86_64.whl size=46402 sha256=a8606d8366225dcd77d0ef69fd75b3e7b4317750e1ca340f19192ce3b99f306a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/6f/33/92bb9a4b4562a60ba6a80cedbab8907e48bc7a8b1f369ea0ae\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: jellyfish\n",
            "Successfully installed jellyfish-0.6.1\n",
            "Collecting unicodecsv\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-cp27-none-any.whl size=10768 sha256=1b566fa4687b255e11c1e53451d8e9607c6c92a208af6f9fd2a9557b09f548a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv\n",
            "Successfully installed unicodecsv-0.14.1\n",
            "Collecting keras.models\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/a5/4d1dd4a1d31c56a28e32441404c01694faa13d384f7d679987eb16a0456e/keras-models-0.0.7.tar.gz\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: np_utils in /usr/local/lib/python2.7/dist-packages (0.5.10.0)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python2.7/dist-packages (from np_utils) (1.16.4)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python2.7/dist-packages (from np_utils) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "divisor: 164.004640656\n",
            "divisor2: 6031.62953937\n",
            "divisor3: 4047.3441906\n",
            "elems_per_fold: \n",
            "1165\n",
            "total chars: 23, target chars: 24\n",
            "indices_char\n",
            "{0: u'\\xa2', 1: u'\\xa3', 2: u'\\xa4', 3: u'\\xa5', 4: u'\\xa6', 5: u'\\xa7', 6: u'\\xa8', 7: u'\\xa9', 8: u'\\xaa', 9: u'\\xab', 10: u'\\xac', 11: u'\\xad', 12: u'\\xae', 13: u'\\xaf', 14: u'\\xb0', 15: u'\\xb1', 16: u'\\xb2', 17: u'\\xb3', 18: u'\\xb4', 19: u'\\xb5', 20: u'\\xb6', 21: u'\\xb7', 22: u'\\xb8'}\n",
            "target_indices_char\n",
            "{0: u'!', 1: u'\\xa2', 2: u'\\xa3', 3: u'\\xa4', 4: u'\\xa5', 5: u'\\xa6', 6: u'\\xa7', 7: u'\\xa8', 8: u'\\xa9', 9: u'\\xaa', 10: u'\\xab', 11: u'\\xac', 12: u'\\xad', 13: u'\\xae', 14: u'\\xaf', 15: u'\\xb0', 16: u'\\xb1', 17: u'\\xb2', 18: u'\\xb3', 19: u'\\xb4', 20: u'\\xb5', 21: u'\\xb6', 22: u'\\xb7', 23: u'\\xb8'}\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "2\n",
            "3\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "4\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "5\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "6\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "7\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "8\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "9\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "10\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "11\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "12\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "13\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "14\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "15\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "16\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "17\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "18\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "19\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "20\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "21\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "22\n",
            "23\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "24\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "25\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "26\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "27\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "28\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "29\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "30\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "31\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "32\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "33\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "34\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "35\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "36\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "37\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "38\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "39\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "40\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "41\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "42\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "43\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "44\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "45\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "46\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "47\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "48\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "49\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "50\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "51\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "52\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "53\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "54\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "55\n",
            "! predicted, end case\n",
            "56\n",
            "! predicted, end case\n",
            "57\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "58\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "59\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "60\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "61\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "62\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "63\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "64\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "65\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "66\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "67\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "68\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "69\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "70\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "71\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "72\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "73\n",
            "74\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "75\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "76\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "77\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "78\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "79\n",
            "80\n",
            "! predicted, end case\n",
            "81\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "82\n",
            "83\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "84\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "85\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "86\n",
            "! predicted, end case\n",
            "87\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "88\n",
            "! predicted, end case\n",
            "89\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "90\n",
            "91\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "92\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "93\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "94\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "95\n",
            "! predicted, end case\n",
            "96\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "97\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "98\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "99\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "100\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "101\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "102\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "103\n",
            "104\n",
            "! predicted, end case\n",
            "105\n",
            "106\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "107\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "108\n",
            "109\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "110\n",
            "! predicted, end case\n",
            "111\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "112\n",
            "! predicted, end case\n",
            "113\n",
            "! predicted, end case\n",
            "114\n",
            "! predicted, end case\n",
            "115\n",
            "116\n",
            "117\n",
            "! predicted, end case\n",
            "118\n",
            "! predicted, end case\n",
            "119\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "120\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "121\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "122\n",
            "! predicted, end case\n",
            "123\n",
            "! predicted, end case\n",
            "124\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "125\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "126\n",
            "127\n",
            "! predicted, end case\n",
            "128\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "133\n",
            "134\n",
            "135\n",
            "! predicted, end case\n",
            "136\n",
            "! predicted, end case\n",
            "137\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "138\n",
            "! predicted, end case\n",
            "139\n",
            "! predicted, end case\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "! predicted, end case\n",
            "144\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "145\n",
            "146\n",
            "147\n",
            "! predicted, end case\n",
            "148\n",
            "! predicted, end case\n",
            "149\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "150\n",
            "! predicted, end case\n",
            "151\n",
            "152\n",
            "153\n",
            "! predicted, end case\n",
            "154\n",
            "155\n",
            "! predicted, end case\n",
            "156\n",
            "157\n",
            "! predicted, end case\n",
            "158\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "159\n",
            "! predicted, end case\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "! predicted, end case\n",
            "164\n",
            "! predicted, end case\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "! predicted, end case\n",
            "173\n",
            "! predicted, end case\n",
            "174\n",
            "175\n",
            "176\n",
            "! predicted, end case\n",
            "177\n",
            "178\n",
            "! predicted, end case\n",
            "179\n",
            "180\n",
            "! predicted, end case\n",
            "181\n",
            "! predicted, end case\n",
            "182\n",
            "183\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "184\n",
            "! predicted, end case\n",
            "185\n",
            "! predicted, end case\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "! predicted, end case\n",
            "191\n",
            "192\n",
            "! predicted, end case\n",
            "193\n",
            "194\n",
            "! predicted, end case\n",
            "! predicted, end case\n",
            "195\n",
            "! predicted, end case\n",
            "196\n",
            "197\n",
            "! predicted, end case\n",
            "198\n",
            "199\n",
            "process finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpO8m7pFlbGL"
      },
      "source": [
        "##**Code: calculate accuracy on next event**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyLukTe1uFJ_"
      },
      "source": [
        "\n",
        "\n",
        "<font color='orange'>- In order to access the latest model constructed, had to type the last model from the output_files --> models map, would be handy that the code can find this latest model automatically with for example code like: 'model_{epoch:02d}-{val_loss:.2f}.h5' </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIgmUBfilgZP",
        "outputId": "600cd88a-7c07-4f0b-e6e6-715e99e628f3"
      },
      "source": [
        "'''\n",
        "this script takes as input the output of evaluate_suffix_and_remaining_time.py\n",
        "therefore, the latter needs to be executed first\n",
        "\n",
        "Author: Niek Tax\n",
        "'''\n",
        "!pip install unicodecsv\n",
        "\n",
        "from __future__ import division\n",
        "import unicodecsv\n",
        "\n",
        "eventlog = \"edxpddf.csv\"\n",
        "csvfile = open('/content/drive/MyDrive/content/code/output_files/results/suffix_and_remaining_time_%s' % eventlog, 'r')\n",
        "r = unicodecsv.reader(csvfile ,encoding='utf-8')\n",
        "r.next() # header\n",
        "vals = dict()\n",
        "for row in r:\n",
        "    l = list()\n",
        "    if row[0] in vals.keys():\n",
        "        l = vals.get(row[0])\n",
        "    if len(row[2])==0 and len(row[3])==0:\n",
        "        l.append(1)\n",
        "    elif len(row[2])==0 and len(row[3])>0:\n",
        "        l.append(0)\n",
        "    elif len(row[2])>0 and len(row[3])==0:\n",
        "        l.append(0)\n",
        "    else:\n",
        "        l.append(int(row[2][0]==row[3][0]))\n",
        "    vals[row[0]] = l\n",
        "    #print(vals)\n",
        "    \n",
        "l2 = list()\n",
        "for k in vals.keys():\n",
        "    #print('{}: {}'.format(k, vals[k]))\n",
        "    l2.extend(vals[k])\n",
        "    res = sum(vals[k])/len(vals[k])\n",
        "    print('{}: {}'.format(k, res))\n",
        "\n",
        "print('total: {}'.format(sum(l2)/len(l2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unicodecsv in /usr/local/lib/python2.7/dist-packages (0.14.1)\n",
            "3922: 0.666666666667\n",
            "3923: 0.555555555556\n",
            "3927: 0.6\n",
            "3924: 0.672839506173\n",
            "3928: 0.5625\n",
            "3929: 0.549019607843\n",
            "4472: 0.640522875817\n",
            "4027: 0.616666666667\n",
            "4024: 0.645161290323\n",
            "4300: 0.583333333333\n",
            "4023: 0.703703703704\n",
            "4020: 1.0\n",
            "4021: 0.641025641026\n",
            "4787: 0.4\n",
            "4307: 0.673469387755\n",
            "4785: 0.583333333333\n",
            "4370: 0.525\n",
            "4781: 0.770642201835\n",
            "3710: 0.64\n",
            "3557: 1.0\n",
            "3259: 1.0\n",
            "3555: 0.6375\n",
            "4371: 0.5\n",
            "3553: 0.68\n",
            "3552: 0.808219178082\n",
            "3551: 0.661538461538\n",
            "4509: 0.307692307692\n",
            "4145: 0.625\n",
            "3251: 1.0\n",
            "3252: 0.363636363636\n",
            "3253: 0.692307692308\n",
            "3255: 0.558823529412\n",
            "3256: 0.67816091954\n",
            "3257: 1.0\n",
            "3856: 0.575\n",
            "3570: 0.0\n",
            "3854: 0.666666666667\n",
            "3855: 0.6\n",
            "3852: 0.0\n",
            "3850: 0.612244897959\n",
            "3851: 0.0\n",
            "3997: 0.48275862069\n",
            "3996: 0.631578947368\n",
            "3995: 0.0\n",
            "4375: 0.708571428571\n",
            "3993: 0.347826086957\n",
            "3992: 0.525510204082\n",
            "3858: 0.75\n",
            "3990: 0.483870967742\n",
            "4839: 0.75\n",
            "4822: 0.833333333333\n",
            "3719: 0.714285714286\n",
            "3991: 0.571428571429\n",
            "4684: 0.652173913043\n",
            "4753: 0.5\n",
            "3333: 0.794117647059\n",
            "4814: 0.734693877551\n",
            "3330: 0.783783783784\n",
            "4579: 0.615384615385\n",
            "3336: 1.0\n",
            "3335: 0.533333333333\n",
            "3334: 0.617647058824\n",
            "4170: 0.5\n",
            "3339: 0.579545454545\n",
            "3338: 0.75\n",
            "4386: 0.567164179104\n",
            "4732: 0.632978723404\n",
            "4733: 0.0\n",
            "4737: 0.714285714286\n",
            "4734: 0.766666666667\n",
            "4571: 0.62962962963\n",
            "4738: 0.666666666667\n",
            "4527: 0.333333333333\n",
            "4526: 0.592592592593\n",
            "4304: 0.719101123596\n",
            "4833: 1.0\n",
            "4575: 1.0\n",
            "3773: 0.133333333333\n",
            "3772: 0.756756756757\n",
            "3771: 0.666666666667\n",
            "3770: 0.758064516129\n",
            "4841: 0.690909090909\n",
            "3775: 0.565789473684\n",
            "4321: 0.612903225806\n",
            "4178: 0.55223880597\n",
            "4323: 0.620689655172\n",
            "3778: 0.857142857143\n",
            "4325: 0.613636363636\n",
            "4324: 0.571428571429\n",
            "4327: 0.529411764706\n",
            "4326: 0.363636363636\n",
            "4655: 0.833333333333\n",
            "3678: 0.541666666667\n",
            "3679: 0.571428571429\n",
            "4651: 0.601941747573\n",
            "4558: 0.551282051282\n",
            "4559: 0.52\n",
            "3672: 0.666666666667\n",
            "3673: 0.5\n",
            "3670: 0.666666666667\n",
            "4520: 0.0\n",
            "4659: 0.611111111111\n",
            "4658: 0.689393939394\n",
            "3674: 0.659793814433\n",
            "4411: 1.0\n",
            "3386: 0.698795180723\n",
            "3387: 0.84375\n",
            "3385: 0.686813186813\n",
            "3382: 1.0\n",
            "3748: 0.542372881356\n",
            "3380: 0.25\n",
            "3381: 0.285714285714\n",
            "4475: 0.375\n",
            "3749: 0.6\n",
            "3388: 0.68\n",
            "3389: 0.75\n",
            "4758: 1.0\n",
            "4477: 1.0\n",
            "4476: 0.545454545455\n",
            "3580: 1.0\n",
            "3581: 0.722222222222\n",
            "3582: 0.676470588235\n",
            "3583: 0.6875\n",
            "3584: 0.6\n",
            "4210: 0.48275862069\n",
            "3586: 0.516129032258\n",
            "3556: 0.666666666667\n",
            "3588: 0.672727272727\n",
            "3589: 0.611111111111\n",
            "4195: 0.4\n",
            "4219: 0.5\n",
            "4218: 0.428571428571\n",
            "4190: 0.655172413793\n",
            "3956: 0.521739130435\n",
            "4110: 0.5\n",
            "4309: 0.573033707865\n",
            "4358: 1.0\n",
            "4354: 0.0\n",
            "4149: 0.637931034483\n",
            "4356: 0.789473684211\n",
            "4350: 0.5\n",
            "4351: 0.869565217391\n",
            "4352: 0.875\n",
            "4148: 1.0\n",
            "4620: 0.5\n",
            "4428: 0.691275167785\n",
            "4429: 0.5\n",
            "4817: 0.375\n",
            "4816: 0.604938271605\n",
            "4483: 0.547945205479\n",
            "4117: 0.518518518519\n",
            "4422: 0.7\n",
            "4306: 0.777777777778\n",
            "4421: 1.0\n",
            "4426: 0.668674698795\n",
            "4427: 0.553191489362\n",
            "4424: 0.692307692308\n",
            "4766: 0.5\n",
            "4112: 0.656\n",
            "4847: 0.5\n",
            "4113: 1.0\n",
            "4146: 0.5\n",
            "3809: 0.454545454545\n",
            "3808: 0.509433962264\n",
            "4081: 0.645161290323\n",
            "4854: 0.333333333333\n",
            "3806: 0.579487179487\n",
            "3801: 0.6\n",
            "3800: 0.5\n",
            "3803: 0.5\n",
            "3802: 0.71875\n",
            "4101: 0.5\n",
            "4100: 0.764705882353\n",
            "4103: 0.571428571429\n",
            "4102: 0.56\n",
            "3298: 0.666666666667\n",
            "3559: 0.63829787234\n",
            "4107: 1.0\n",
            "4106: 0.384615384615\n",
            "3294: 0.666666666667\n",
            "3295: 1.0\n",
            "3558: 0.714285714286\n",
            "3290: 0.567567567568\n",
            "3293: 0.526315789474\n",
            "3452: 0.888888888889\n",
            "3453: 0.466666666667\n",
            "3519: 0.72131147541\n",
            "3518: 0.5\n",
            "3456: 0.708333333333\n",
            "3457: 0.620689655172\n",
            "3454: 0.333333333333\n",
            "3455: 0.8125\n",
            "3513: 0.52380952381\n",
            "3458: 0.866666666667\n",
            "3510: 0.666666666667\n",
            "3516: 0.5\n",
            "3515: 0.633333333333\n",
            "4601: 0.428571428571\n",
            "3898: 0.615384615385\n",
            "3892: 0.509433962264\n",
            "3893: 0.65\n",
            "3890: 0.532258064516\n",
            "3891: 0.5\n",
            "4886: 0.444444444444\n",
            "3897: 0.777777777778\n",
            "3894: 0.692307692308\n",
            "3895: 0.6875\n",
            "3508: 0.833333333333\n",
            "4819: 0.633333333333\n",
            "4398: 0.620689655172\n",
            "4759: 0.679487179487\n",
            "3998: 0.25\n",
            "4338: 0.5\n",
            "4776: 0.653846153846\n",
            "4777: 0.444444444444\n",
            "4002: 0.821428571429\n",
            "4775: 1.0\n",
            "4773: 0.736842105263\n",
            "4771: 0.384615384615\n",
            "4778: 0.654411764706\n",
            "4298: 0.789473684211\n",
            "4174: 0.666666666667\n",
            "4175: 0.613402061856\n",
            "4177: 0.5\n",
            "4079: 0.520547945205\n",
            "4171: 0.631578947368\n",
            "4172: 1.0\n",
            "4075: 0.333333333333\n",
            "4074: 0.677215189873\n",
            "4077: 0.589743589744\n",
            "4076: 0.555555555556\n",
            "4071: 0.66393442623\n",
            "4070: 0.709183673469\n",
            "4295: 0.6\n",
            "3965: 0.625\n",
            "4265: 0.666666666667\n",
            "3963: 0.696\n",
            "3961: 0.384615384615\n",
            "4824: 0.644171779141\n",
            "4297: 0.586956521739\n",
            "3969: 1.0\n",
            "4305: 0.716129032258\n",
            "4296: 0.571428571429\n",
            "3638: 1.0\n",
            "3639: 0.571428571429\n",
            "3637: 0.72\n",
            "3635: 0.75\n",
            "3633: 0.58064516129\n",
            "3630: 0.65625\n",
            "4618: 0.555555555556\n",
            "4266: 0.640718562874\n",
            "3506: 0.619047619048\n",
            "4611: 0.571428571429\n",
            "4610: 0.5\n",
            "4613: 0.384615384615\n",
            "4615: 0.666666666667\n",
            "4392: 0.611111111111\n",
            "4617: 0.5\n",
            "3348: 0.6\n",
            "4810: 0.611111111111\n",
            "3342: 0.728571428571\n",
            "3343: 0.533333333333\n",
            "4638: 0.794117647059\n",
            "3341: 0.618644067797\n",
            "3346: 0.727272727273\n",
            "3347: 0.6875\n",
            "3344: 0.664864864865\n",
            "3345: 0.598214285714\n",
            "4267: 1.0\n",
            "4826: 0.531914893617\n",
            "4863: 0.630434782609\n",
            "4614: 0.4\n",
            "4259: 0.637931034483\n",
            "4258: 0.6\n",
            "4883: 0.793333333333\n",
            "4314: 0.5\n",
            "4317: 0.451612903226\n",
            "4318: 0.5\n",
            "4250: 1.0\n",
            "4252: 0.727272727273\n",
            "4254: 0.333333333333\n",
            "4505: 0.622222222222\n",
            "4504: 1.0\n",
            "3708: 0.761904761905\n",
            "4506: 0.619047619048\n",
            "4501: 0.75\n",
            "4500: 0.872832369942\n",
            "4468: 0.666666666667\n",
            "4502: 0.571428571429\n",
            "3702: 0.75\n",
            "3703: 0.5\n",
            "3701: 0.833333333333\n",
            "3706: 0.627450980392\n",
            "3707: 0.681818181818\n",
            "3704: 0.357142857143\n",
            "4668: 0.641509433962\n",
            "4669: 0.461538461538\n",
            "4664: 0.666666666667\n",
            "4665: 1.0\n",
            "4666: 0.838709677419\n",
            "4667: 0.764705882353\n",
            "4269: 0.833333333333\n",
            "4663: 0.583333333333\n",
            "4328: 0.35\n",
            "4815: 0.590909090909\n",
            "4672: 0.711538461538\n",
            "4261: 0.65306122449\n",
            "4104: 0.733333333333\n",
            "3416: 0.581395348837\n",
            "3417: 0.48275862069\n",
            "3414: 0.630208333333\n",
            "3415: 0.5\n",
            "3412: 0.666666666667\n",
            "3413: 0.578947368421\n",
            "3410: 0.594405594406\n",
            "3411: 0.636363636364\n",
            "4225: 0.513888888889\n",
            "4226: 0.375\n",
            "4152: 0.631578947368\n",
            "4221: 0.0\n",
            "3418: 0.25\n",
            "3419: 0.4375\n",
            "4389: 0.833333333333\n",
            "4388: 0.611940298507\n",
            "3799: 0.625\n",
            "3798: 0.608695652174\n",
            "4593: 0.745098039216\n",
            "3794: 0.378048780488\n",
            "3796: 0.535714285714\n",
            "3791: 0.683333333333\n",
            "3790: 0.785714285714\n",
            "3793: 0.6\n",
            "3792: 0.0\n",
            "4268: 0.52380952381\n",
            "4849: 0.45\n",
            "4180: 0.333333333333\n",
            "4862: 0.591836734694\n",
            "4848: 0.675\n",
            "4230: 0.651685393258\n",
            "4183: 0.454545454545\n",
            "4864: 0.604651162791\n",
            "4597: 0.48\n",
            "3834: 0.75\n",
            "3836: 0.709677419355\n",
            "3837: 0.546875\n",
            "3830: 0.818181818182\n",
            "3831: 0.555555555556\n",
            "3935: 0.666666666667\n",
            "3937: 0.4\n",
            "3936: 0.368421052632\n",
            "3838: 0.675675675676\n",
            "3839: 0.791666666667\n",
            "3933: 1.0\n",
            "3932: 0.516129032258\n",
            "4031: 0.571428571429\n",
            "4035: 0.710526315789\n",
            "4037: 0.545454545455\n",
            "4036: 0.59375\n",
            "4038: 0.68\n",
            "4870: 0.557692307692\n",
            "4138: 0.659574468085\n",
            "3563: 0.571428571429\n",
            "3248: 0.636363636364\n",
            "3566: 0.393939393939\n",
            "3567: 0.657894736842\n",
            "3565: 0.648648648649\n",
            "3243: 0.578125\n",
            "3242: 0.623655913978\n",
            "3241: 0.698795180723\n",
            "3240: 0.824175824176\n",
            "3247: 0.0\n",
            "3246: 0.729411764706\n",
            "3245: 0.625698324022\n",
            "3244: 0.5\n",
            "3840: 0.622222222222\n",
            "3843: 0.666666666667\n",
            "3842: 0.72131147541\n",
            "3845: 0.623529411765\n",
            "3844: 0.658536585366\n",
            "3847: 0.706896551724\n",
            "3846: 0.65625\n",
            "3849: 0.25\n",
            "3848: 0.428571428571\n",
            "4825: 0.6\n",
            "4489: 0.533333333333\n",
            "4310: 1.0\n",
            "4311: 0.84693877551\n",
            "4879: 0.751592356688\n",
            "4878: 0.666666666667\n",
            "4818: 0.740740740741\n",
            "3307: 0.666666666667\n",
            "3304: 0.51724137931\n",
            "3305: 0.888888888889\n",
            "3302: 0.0\n",
            "3300: 0.727272727273\n",
            "3301: 1.0\n",
            "4044: 0.0\n",
            "4467: 0.571428571429\n",
            "4046: 0.639344262295\n",
            "4047: 0.551724137931\n",
            "4729: 0.614583333333\n",
            "4041: 1.0\n",
            "4042: 0.533333333333\n",
            "4725: 1.0\n",
            "3451: 0.666666666667\n",
            "4727: 0.716666666667\n",
            "4721: 0.333333333333\n",
            "4723: 0.315789473684\n",
            "4438: 0.647540983607\n",
            "4689: 0.739130434783\n",
            "4319: 0.565217391304\n",
            "3712: 0.791666666667\n",
            "4463: 0.8\n",
            "3665: 0.833333333333\n",
            "3666: 0.767441860465\n",
            "3663: 0.733333333333\n",
            "3662: 0.888888888889\n",
            "3746: 0.4\n",
            "3747: 0.409090909091\n",
            "3744: 0.591836734694\n",
            "3745: 0.5\n",
            "3743: 0.666666666667\n",
            "3741: 0.806818181818\n",
            "4549: 0.652173913043\n",
            "4548: 1.0\n",
            "4622: 1.0\n",
            "4625: 0.72\n",
            "4626: 0.553191489362\n",
            "4627: 0.597402597403\n",
            "4541: 0.671875\n",
            "4238: 0.615384615385\n",
            "4544: 0.625\n",
            "4547: 0.857142857143\n",
            "3390: 0.642857142857\n",
            "3393: 0.421052631579\n",
            "3392: 1.0\n",
            "3395: 0.706666666667\n",
            "3394: 0.633333333333\n",
            "3397: 0.545454545455\n",
            "3396: 1.0\n",
            "3399: 0.0\n",
            "3398: 0.5\n",
            "4143: 1.0\n",
            "4507: 0.653846153846\n",
            "4534: 0.5\n",
            "4535: 0.333333333333\n",
            "4536: 0.388888888889\n",
            "4188: 0.615384615385\n",
            "4263: 0.684210526316\n",
            "4264: 0.285714285714\n",
            "3596: 0.532467532468\n",
            "3595: 0.622222222222\n",
            "3594: 0.565934065934\n",
            "4181: 0.761904761905\n",
            "4142: 0.590909090909\n",
            "3599: 0.647727272727\n",
            "3598: 0.611842105263\n",
            "4184: 0.578947368421\n",
            "4187: 0.540983606557\n",
            "4186: 1.0\n",
            "4435: 0.875\n",
            "4434: 0.679738562092\n",
            "3713: 0.634020618557\n",
            "4436: 0.5\n",
            "4431: 0.590909090909\n",
            "4430: 0.654320987654\n",
            "4433: 0.652173913043\n",
            "4348: 1.0\n",
            "4347: 0.5\n",
            "4345: 1.0\n",
            "4344: 0.634615384615\n",
            "4439: 0.5\n",
            "4342: 0.5\n",
            "4341: 0.428571428571\n",
            "4340: 0.6\n",
            "3698: 0.666666666667\n",
            "3699: 0.8\n",
            "4531: 0.751724137931\n",
            "4465: 1.0\n",
            "4538: 0.636363636364\n",
            "4657: 0.571428571429\n",
            "3690: 0.333333333333\n",
            "3691: 0.333333333333\n",
            "3692: 0.0\n",
            "3693: 0.555555555556\n",
            "4530: 0.75\n",
            "3695: 0.5\n",
            "3696: 0.666666666667\n",
            "3697: 0.72\n",
            "4792: 0.62\n",
            "4320: 0.4\n",
            "4894: 0.712871287129\n",
            "3878: 0.538461538462\n",
            "3879: 1.0\n",
            "4322: 0.642857142857\n",
            "3870: 0.666666666667\n",
            "3871: 0.583333333333\n",
            "3872: 0.745762711864\n",
            "3873: 0.791666666667\n",
            "3874: 0.688524590164\n",
            "4150: 0.536231884058\n",
            "3876: 0.677966101695\n",
            "3877: 0.666666666667\n",
            "3526: 0.75\n",
            "3524: 1.0\n",
            "3525: 0.75641025641\n",
            "3522: 0.333333333333\n",
            "3523: 0.606741573034\n",
            "3520: 0.648148148148\n",
            "3288: 1.0\n",
            "3287: 0.666666666667\n",
            "3286: 0.472727272727\n",
            "3285: 0.45\n",
            "3283: 0.538461538462\n",
            "3282: 0.545454545455\n",
            "3528: 0.583333333333\n",
            "3529: 0.5\n",
            "4760: 0.756756756757\n",
            "3887: 0.636363636364\n",
            "3886: 0.666666666667\n",
            "3448: 0.727272727273\n",
            "3883: 0.666666666667\n",
            "3882: 0.5\n",
            "3445: 0.5\n",
            "3444: 0.5\n",
            "4214: 0.549668874172\n",
            "3446: 0.0\n",
            "3441: 1.0\n",
            "3440: 0.652631578947\n",
            "3443: 0.5\n",
            "4656: 0.333333333333\n",
            "4158: 0.780487804878\n",
            "3904: 0.621052631579\n",
            "3906: 0.625\n",
            "3907: 0.678787878788\n",
            "3901: 0.756097560976\n",
            "3902: 0.448717948718\n",
            "3903: 0.285714285714\n",
            "4217: 0.4\n",
            "4590: 0.519480519481\n",
            "4761: 0.714285714286\n",
            "4556: 0.636363636364\n",
            "4762: 0.59375\n",
            "4765: 0.736842105263\n",
            "4764: 0.272727272727\n",
            "4767: 0.618784530387\n",
            "4557: 0.5\n",
            "4768: 0.660550458716\n",
            "4554: 0.666666666667\n",
            "4199: 0.5\n",
            "3232: 0.6\n",
            "3233: 0.933333333333\n",
            "3230: 0.5\n",
            "3231: 0.642857142857\n",
            "3236: 0.659574468085\n",
            "3237: 0.614285714286\n",
            "3234: 0.455882352941\n",
            "3235: 0.625\n",
            "4000: 0.4\n",
            "4001: 0.604316546763\n",
            "3238: 0.613445378151\n",
            "3239: 0.538461538462\n",
            "4004: 0.166666666667\n",
            "4005: 0.5\n",
            "4006: 0.730769230769\n",
            "4007: 1.0\n",
            "3970: 1.0\n",
            "3972: 0.625\n",
            "4555: 0.697368421053\n",
            "3974: 0.25\n",
            "3977: 0.333333333333\n",
            "3976: 0.571428571429\n",
            "3979: 1.0\n",
            "3978: 0.537313432836\n",
            "4551: 0.857142857143\n",
            "3629: 0.708333333333\n",
            "3628: 0.625\n",
            "3621: 0.822222222222\n",
            "3620: 0.5\n",
            "3622: 0.454545454545\n",
            "3625: 0.697368421053\n",
            "3624: 0.846153846154\n",
            "3626: 0.788732394366\n",
            "3355: 1.0\n",
            "3354: 0.614583333333\n",
            "3357: 0.609756097561\n",
            "3356: 0.673469387755\n",
            "3351: 0.55737704918\n",
            "4828: 0.857142857143\n",
            "3353: 0.633333333333\n",
            "3352: 0.633333333333\n",
            "3470: 0.708955223881\n",
            "3359: 0.694736842105\n",
            "3578: 0.333333333333\n",
            "4714: 0.333333333333\n",
            "4716: 0.0\n",
            "4215: 0.656565656566\n",
            "4710: 0.602564102564\n",
            "4284: 0.736842105263\n",
            "4712: 0.461538461538\n",
            "4713: 0.272727272727\n",
            "4285: 0.775862068966\n",
            "4718: 0.601769911504\n",
            "4503: 0.666666666667\n",
            "4122: 0.515151515152\n",
            "4121: 0.605504587156\n",
            "3715: 0.714285714286\n",
            "3714: 0.731034482759\n",
            "4301: 0.585365853659\n",
            "3716: 0.655737704918\n",
            "3711: 0.9\n",
            "3289: 1.0\n",
            "3498: 0.673913043478\n",
            "3499: 0.666666666667\n",
            "3496: 0.576923076923\n",
            "3478: 0.775\n",
            "3494: 0.72619047619\n",
            "3495: 0.0\n",
            "3492: 0.384615384615\n",
            "3493: 0.666666666667\n",
            "3490: 0.553846153846\n",
            "3491: 0.560975609756\n",
            "4570: 0.702898550725\n",
            "4478: 0.684931506849\n",
            "4572: 0.453125\n",
            "4573: 0.6\n",
            "3658: 0.543209876543\n",
            "3659: 0.577586206897\n",
            "4576: 0.526315789474\n",
            "4577: 0.571428571429\n",
            "3654: 0.454545454545\n",
            "4470: 0.573248407643\n",
            "4473: 0.620689655172\n",
            "4124: 0.627906976744\n",
            "3650: 0.625\n",
            "3652: 0.607142857143\n",
            "3653: 0.693548387097\n",
            "4293: 0.541666666667\n",
            "4621: 0.692307692308\n",
            "4838: 0.727272727273\n",
            "4679: 0.5\n",
            "4678: 0.5\n",
            "4677: 0.647058823529\n",
            "4676: 0.612903225806\n",
            "4674: 0.579831932773\n",
            "4673: 1.0\n",
            "4156: 0.5\n",
            "4671: 0.630434782609\n",
            "4670: 0.8\n",
            "3280: 0.6\n",
            "4880: 0.5\n",
            "4881: 0.777777777778\n",
            "4882: 0.0\n",
            "4157: 0.511363636364\n",
            "4885: 0.733333333333\n",
            "4533: 0.5\n",
            "3401: 0.717948717949\n",
            "3400: 0.5\n",
            "3402: 0.561403508772\n",
            "3405: 0.4\n",
            "3404: 0.375\n",
            "3407: 0.5\n",
            "3406: 0.72\n",
            "3409: 0.5\n",
            "4236: 0.764705882353\n",
            "4233: 1.0\n",
            "4232: 0.666666666667\n",
            "4231: 0.601226993865\n",
            "3880: 0.639097744361\n",
            "4875: 1.0\n",
            "4877: 0.73\n",
            "4876: 0.593220338983\n",
            "4408: 0.666666666667\n",
            "4409: 0.48275862069\n",
            "4873: 0.583333333333\n",
            "4405: 0.762711864407\n",
            "4406: 0.8\n",
            "4400: 0.714285714286\n",
            "4401: 0.714285714286\n",
            "4403: 0.6875\n",
            "4228: 0.475\n",
            "3948: 0.5\n",
            "3826: 0.681818181818\n",
            "3825: 0.266666666667\n",
            "4532: 1.0\n",
            "3822: 0.788732394366\n",
            "3821: 0.333333333333\n",
            "3820: 0.5\n",
            "3889: 0.333333333333\n",
            "3942: 0.8\n",
            "4397: 0.4375\n",
            "3944: 0.627272727273\n",
            "3945: 0.631578947368\n",
            "3829: 0.62893081761\n",
            "3828: 0.863636363636\n",
            "3276: 0.666666666667\n",
            "3277: 0.59375\n",
            "3275: 0.780487804878\n",
            "3272: 0.727272727273\n",
            "3273: 0.5\n",
            "3270: 0.653846153846\n",
            "4287: 0.68\n",
            "4211: 0.700564971751\n",
            "3278: 0.333333333333\n",
            "3474: 0.671641791045\n",
            "3475: 0.642857142857\n",
            "3476: 0.25\n",
            "3477: 0.833333333333\n",
            "3579: 0.575757575758\n",
            "3471: 0.685393258427\n",
            "3472: 0.740157480315\n",
            "3473: 0.780487804878\n",
            "4123: 0.648648648649\n",
            "3574: 0.611111111111\n",
            "3577: 0.7\n",
            "3576: 0.615384615385\n",
            "3571: 0.0\n",
            "3479: 0.777777777778\n",
            "4125: 0.5\n",
            "3572: 0.0\n",
            "4496: 0.630872483221\n",
            "4494: 0.5\n",
            "4493: 0.693069306931\n",
            "4491: 0.8\n",
            "4490: 0.661016949153\n",
            "4223: 1.0\n",
            "4499: 0.714285714286\n",
            "4498: 0.518518518519\n",
            "4128: 0.416666666667\n",
            "4550: 0.477272727273\n",
            "4705: 0.666666666667\n",
            "3319: 0.166666666667\n",
            "3311: 0.416666666667\n",
            "3310: 0.461538461538\n",
            "3313: 0.58064516129\n",
            "3312: 0.333333333333\n",
            "3315: 0.833333333333\n",
            "3314: 0.666666666667\n",
            "3317: 0.594594594595\n",
            "3316: 0.703703703704\n",
            "4057: 0.6\n",
            "4056: 0.0\n",
            "4054: 0.0\n",
            "4053: 0.5\n",
            "4051: 0.9\n",
            "4050: 0.444444444444\n",
            "4750: 0.6\n",
            "4751: 0.569230769231\n",
            "4282: 0.730769230769\n",
            "4754: 0.725806451613\n",
            "4755: 0.569620253165\n",
            "4059: 1.0\n",
            "4058: 1.0\n",
            "4382: 1.0\n",
            "3610: 0.72\n",
            "3611: 0.772727272727\n",
            "3612: 0.739130434783\n",
            "3613: 0.548387096774\n",
            "3615: 0.25\n",
            "3616: 0.571428571429\n",
            "3617: 0.444444444444\n",
            "3618: 1.0\n",
            "3619: 0.25\n",
            "3753: 1.0\n",
            "3755: 0.742424242424\n",
            "3754: 0.458333333333\n",
            "3757: 0.5\n",
            "3756: 0.885714285714\n",
            "4633: 0.0\n",
            "4631: 0.557692307692\n",
            "4637: 0.768421052632\n",
            "4709: 0.662721893491\n",
            "4635: 0.352941176471\n",
            "4355: 0.631578947368\n",
            "4639: 0.6\n",
            "4384: 0.3125\n",
            "4360: 0.375\n",
            "4363: 0.635294117647\n",
            "4280: 0.708333333333\n",
            "3365: 0.857142857143\n",
            "3367: 0.832402234637\n",
            "3361: 0.714285714286\n",
            "3362: 0.622950819672\n",
            "4869: 0.5\n",
            "4353: 0.666666666667\n",
            "4273: 0.631578947368\n",
            "4272: 0.4\n",
            "4271: 0.625\n",
            "4270: 0.375\n",
            "4277: 0.703703703704\n",
            "4281: 0.777777777778\n",
            "4274: 0.714285714286\n",
            "4369: 0.628571428571\n",
            "4279: 0.703703703704\n",
            "4220: 0.508771929825\n",
            "4811: 0.584615384615\n",
            "4440: 0.588235294118\n",
            "4441: 0.571428571429\n",
            "4442: 0.444444444444\n",
            "4443: 0.333333333333\n",
            "3728: 0.655172413793\n",
            "4445: 0.296296296296\n",
            "4447: 0.666666666667\n",
            "3724: 0.625\n",
            "3725: 0.444444444444\n",
            "3726: 0.488888888889\n",
            "3727: 0.555555555556\n",
            "3720: 0.0\n",
            "3721: 0.625\n",
            "4374: 0.814432989691\n",
            "3723: 0.736263736264\n",
            "4686: 0.40625\n",
            "4687: 0.454545454545\n",
            "3689: 0.333333333333\n",
            "3688: 0.549019607843\n",
            "4683: 0.671052631579\n",
            "4680: 0.597222222222\n",
            "4528: 0.633333333333\n",
            "3683: 0.333333333333\n",
            "3682: 0.481481481481\n",
            "3681: 0.474576271186\n",
            "3680: 0.6\n",
            "4523: 0.695652173913\n",
            "3686: 1.0\n",
            "4688: 0.555555555556\n",
            "3684: 0.428571428571\n",
            "4420: 0.771428571429\n",
            "4830: 0.5\n",
            "4080: 0.479338842975\n",
            "3760: 0.5\n",
            "3869: 1.0\n",
            "3868: 0.533333333333\n",
            "4455: 0.675324675325\n",
            "3761: 0.666666666667\n",
            "4088: 0.558139534884\n",
            "3860: 0.733333333333\n",
            "3867: 0.5\n",
            "3866: 0.555555555556\n",
            "3865: 0.526315789474\n",
            "3864: 0.728813559322\n",
            "3531: 0.612903225806\n",
            "3530: 0.25\n",
            "3533: 0.695652173913\n",
            "3535: 0.685185185185\n",
            "3534: 0.714285714286\n",
            "3539: 0.454545454545\n",
            "3538: 0.533333333333\n",
            "4757: 0.0\n",
            "3938: 0.375\n",
            "3438: 0.5\n",
            "3439: 0.571428571429\n",
            "4216: 0.681415929204\n",
            "3431: 0.5\n",
            "4339: 0.761904761905\n",
            "3436: 0.6\n",
            "3437: 0.461538461538\n",
            "4592: 0.464285714286\n",
            "3768: 0.813333333333\n",
            "4466: 0.8\n",
            "4596: 0.769230769231\n",
            "4120: 0.5\n",
            "4594: 0.673469387755\n",
            "4337: 0.69\n",
            "4844: 0.467289719626\n",
            "4845: 0.590163934426\n",
            "4598: 0.68\n",
            "4599: 0.662068965517\n",
            "4334: 0.705882352941\n",
            "4843: 0.280701754386\n",
            "3917: 0.540816326531\n",
            "4008: 0.666666666667\n",
            "4335: 0.774647887324\n",
            "3910: 0.691056910569\n",
            "3931: 0.62\n",
            "4693: 0.830188679245\n",
            "4333: 0.5\n",
            "4127: 1.0\n",
            "4798: 0.702702702703\n",
            "4799: 0.6\n",
            "4794: 0.709090909091\n",
            "4163: 0.909090909091\n",
            "4796: 0.675\n",
            "4331: 0.777777777778\n",
            "4790: 0.636363636364\n",
            "3587: 0.444444444444\n",
            "4793: 0.634146341463\n",
            "3544: 0.333333333333\n",
            "3545: 1.0\n",
            "3546: 0.78125\n",
            "3547: 0.697142857143\n",
            "3540: 0.720930232558\n",
            "3541: 0.65\n",
            "3542: 0.711111111111\n",
            "3543: 0.661971830986\n",
            "4012: 1.0\n",
            "4011: 0.636363636364\n",
            "4010: 0.785714285714\n",
            "3548: 0.797872340426\n",
            "3549: 0.666666666667\n",
            "4015: 0.333333333333\n",
            "4014: 0.7\n",
            "4126: 0.647058823529\n",
            "4560: 0.409090909091\n",
            "3988: 0.75\n",
            "3989: 0.4\n",
            "4853: 0.583333333333\n",
            "3984: 0.75\n",
            "3985: 0.675675675676\n",
            "3980: 0.666666666667\n",
            "3981: 0.7\n",
            "4003: 0.53488372093\n",
            "3758: 0.583333333333\n",
            "4564: 0.636363636364\n",
            "3320: 0.55\n",
            "3321: 0.694444444444\n",
            "3322: 0.35\n",
            "3323: 0.5\n",
            "3325: 0.483050847458\n",
            "3326: 0.62962962963\n",
            "3327: 1.0\n",
            "3329: 0.724358974359\n",
            "4568: 0.680412371134\n",
            "4167: 0.0\n",
            "4703: 0.5\n",
            "4702: 0.688524590164\n",
            "4701: 0.598765432099\n",
            "4700: 0.625\n",
            "4580: 1.0\n",
            "4140: 0.627906976744\n",
            "4708: 0.730434782609\n",
            "4823: 0.52380952381\n",
            "3751: 0.5\n",
            "4836: 0.662337662338\n",
            "3750: 0.555555555556\n",
            "4741: 0.578947368421\n",
            "4182: 1.0\n",
            "3489: 0.690909090909\n",
            "3488: 0.615384615385\n",
            "3762: 0.607843137255\n",
            "3763: 0.692307692308\n",
            "3764: 0.827586206897\n",
            "3765: 0.558823529412\n",
            "3766: 0.740740740741\n",
            "3767: 0.635294117647\n",
            "3481: 0.5\n",
            "3480: 0.59375\n",
            "3483: 0.875\n",
            "3482: 0.333333333333\n",
            "3485: 0.407407407407\n",
            "3484: 0.56\n",
            "3487: 0.615384615385\n",
            "3486: 0.528925619835\n",
            "4563: 0.5\n",
            "4562: 0.75\n",
            "4561: 0.428571428571\n",
            "3561: 1.0\n",
            "4567: 0.458333333333\n",
            "4566: 0.692307692308\n",
            "4565: 0.545454545455\n",
            "3648: 0.333333333333\n",
            "3646: 0.5\n",
            "3644: 1.0\n",
            "3643: 0.648148148148\n",
            "3642: 0.8\n",
            "3641: 0.710526315789\n",
            "4806: 0.770833333333\n",
            "4642: 0.777777777778\n",
            "4643: 0.675675675676\n",
            "4641: 0.921348314607\n",
            "4646: 0.670454545455\n",
            "4647: 0.550561797753\n",
            "4130: 0.725\n",
            "4131: 0.739130434783\n",
            "4105: 0.421052631579\n",
            "3299: 0.555555555556\n",
            "4134: 0.666666666667\n",
            "4634: 0.525925925926\n",
            "4896: 0.333333333333\n",
            "4135: 0.681818181818\n",
            "4109: 0.568965517241\n",
            "4208: 0.707317073171\n",
            "4209: 0.5\n",
            "4108: 0.5\n",
            "4206: 0.72\n",
            "4756: 0.587301587302\n",
            "4204: 0.64\n",
            "4205: 0.63829787234\n",
            "4332: 0.25\n",
            "3297: 0.684210526316\n",
            "4804: 0.645161290323\n",
            "4805: 0.662337662338\n",
            "4419: 0.666666666667\n",
            "4418: 0.575342465753\n",
            "4417: 0.58\n",
            "4416: 1.0\n",
            "4415: 0.642857142857\n",
            "4414: 0.62962962963\n",
            "4412: 0.694736842105\n",
            "4082: 0.607142857143\n",
            "4410: 0.0\n",
            "4487: 0.621621621622\n",
            "4812: 0.657534246575\n",
            "4481: 0.7\n",
            "3812: 0.7125\n",
            "3813: 0.7\n",
            "3959: 0.666666666667\n",
            "3958: 0.428571428571\n",
            "3816: 0.511111111111\n",
            "4482: 1.0\n",
            "3814: 0.764705882353\n",
            "3815: 0.75\n",
            "3953: 0.764705882353\n",
            "3952: 0.525\n",
            "3950: 0.428571428571\n",
            "3957: 0.6\n",
            "3450: 0.574074074074\n",
            "3954: 0.790697674419\n",
            "4116: 0.835820895522\n",
            "3260: 0.615384615385\n",
            "3262: 0.666666666667\n",
            "3265: 0.333333333333\n",
            "3264: 0.666666666667\n",
            "3267: 0.730769230769\n",
            "4118: 0.4\n",
            "4119: 0.621428571429\n",
            "3467: 0.558823529412\n",
            "3509: 0.666666666667\n",
            "3465: 0.714285714286\n",
            "3464: 0.5\n",
            "3463: 0.333333333333\n",
            "3462: 0.714285714286\n",
            "3461: 0.0\n",
            "3500: 0.65625\n",
            "3501: 0.416666666667\n",
            "3502: 0.662576687117\n",
            "3503: 0.818181818182\n",
            "3504: 0.470588235294\n",
            "3469: 0.678571428571\n",
            "4292: 0.6\n",
            "4139: 0.768115942029\n",
            "4448: 0.56\n",
            "4797: 0.538461538462\n",
            "4198: 0.428571428571\n",
            "4129: 0.511278195489\n",
            "4485: 0.6\n",
            "4449: 0.509433962264\n",
            "4062: 1.0\n",
            "4749: 0.818181818182\n",
            "4748: 0.6\n",
            "4066: 0.694915254237\n",
            "4067: 1.0\n",
            "4064: 1.0\n",
            "4743: 0.632530120482\n",
            "4742: 0.773109243697\n",
            "4068: 0.672131147541\n",
            "4746: 0.791666666667\n",
            "4744: 0.732484076433\n",
            "4892: 0.627450980392\n",
            "3603: 0.5\n",
            "3602: 0.666666666667\n",
            "3601: 0.516129032258\n",
            "3600: 0.555555555556\n",
            "3607: 0.714285714286\n",
            "3606: 0.528497409326\n",
            "3605: 0.770833333333\n",
            "4399: 0.705882352941\n",
            "3609: 1.0\n",
            "3608: 0.775\n",
            "3780: 0.6\n",
            "4717: 0.777777777778\n",
            "4608: 0.541666666667\n",
            "4609: 0.583333333333\n",
            "4606: 0.56\n",
            "4607: 0.487179487179\n",
            "4604: 0.37037037037\n",
            "4605: 0.0\n",
            "4602: 0.589743589744\n",
            "4213: 0.516129032258\n",
            "3379: 1.0\n",
            "3378: 0.684931506849\n",
            "3377: 1.0\n",
            "3376: 0.75\n",
            "3375: 0.655172413793\n",
            "3373: 0.635135135135\n",
            "3372: 0.672514619883\n",
            "3371: 0.25\n",
            "4362: 0.777777777778\n",
            "4782: 0.625\n",
            "4786: 1.0\n",
            "3788: 0.5\n",
            "4365: 0.567567567568\n",
            "4364: 0.567567567568\n",
            "4367: 0.666666666667\n",
            "4393: 0.666666666667\n",
            "4361: 0.4\n",
            "4288: 0.615384615385\n",
            "4248: 0.807692307692\n",
            "4249: 0.5\n",
            "4247: 0.607843137255\n",
            "4244: 0.5\n",
            "4242: 0.5\n",
            "4243: 0.53488372093\n",
            "4240: 0.5\n",
            "4453: 0.635416666667\n",
            "4452: 0.613636363636\n",
            "4450: 0.75\n",
            "4457: 0.888888888889\n",
            "4396: 0.612244897959\n",
            "3739: 0.333333333333\n",
            "4459: 0.834710743802\n",
            "3734: 0.833333333333\n",
            "3733: 0.551401869159\n",
            "3732: 0.0\n",
            "3731: 0.589743589744\n",
            "3730: 0.666666666667\n",
            "4518: 0.666666666667\n",
            "4519: 0.711111111111\n",
            "4695: 1.0\n",
            "4303: 0.5\n",
            "4512: 0.611111111111\n",
            "4513: 0.65306122449\n",
            "4510: 0.333333333333\n",
            "4516: 0.5\n",
            "4517: 0.622222222222\n",
            "4514: 0.560810810811\n",
            "4515: 0.740157480315\n",
            "4850: 0.454545454545\n",
            "4278: 0.607142857143\n",
            "4093: 0.566037735849\n",
            "4092: 0.461538461538\n",
            "4091: 0.53488372093\n",
            "4090: 1.0\n",
            "4096: 0.515151515152\n",
            "4095: 1.0\n",
            "4099: 0.762376237624\n",
            "4098: 0.68\n",
            "4774: 0.833333333333\n",
            "4803: 1.0\n",
            "4584: 0.654545454545\n",
            "4728: 0.5\n",
            "3782: 0.545454545455\n",
            "3783: 0.714285714286\n",
            "3429: 0.661654135338\n",
            "4586: 0.485714285714\n",
            "3787: 0.6\n",
            "3784: 0.460526315789\n",
            "3422: 0.543859649123\n",
            "3421: 0.666666666667\n",
            "3789: 1.0\n",
            "3427: 0.68306010929\n",
            "3426: 0.75\n",
            "3425: 1.0\n",
            "3424: 0.571428571429\n",
            "4589: 0.611111111111\n",
            "4588: 0.627118644068\n",
            "4859: 0.55\n",
            "4858: 0.0\n",
            "4857: 0.5\n",
            "4444: 0.530303030303\n",
            "4855: 0.75\n",
            "4372: 0.632075471698\n",
            "4581: 0.714285714286\n",
            "4212: 0.75\n",
            "4851: 0.5\n",
            "4582: 0.5\n",
            "total: 0.638811168287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pESmW9fQl6JR"
      },
      "source": [
        "##**Code: evaluate next activity and time**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "szSz-fbml9kW",
        "outputId": "e9f834f2-2f28-4b76-e68e-1a1ddb7efa87"
      },
      "source": [
        "'''\n",
        "this script takes as input the LSTM or RNN weights found by train.py\n",
        "change the path in line 176 of this script to point to the h5 file\n",
        "with LSTM or RNN weights generated by train.py\n",
        "\n",
        "Author: Niek Tax\n",
        "'''\n",
        "\n",
        "!pip install distance\n",
        "!pip install jellyfish\n",
        "!pip install unicodecsv\n",
        "!pip install distance\n",
        "\n",
        "from __future__ import division\n",
        "from keras.models import load_model\n",
        "import csv\n",
        "import copy\n",
        "import numpy as np\n",
        "import distance\n",
        "from itertools import izip\n",
        "from jellyfish._jellyfish import damerau_levenshtein_distance \n",
        "import unicodecsv\n",
        "from sklearn import metrics\n",
        "from math import sqrt\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "eventlog = \"edxpddf.csv\"\n",
        "csvfile = open('/content/drive/MyDrive/content/data/%s' % eventlog, 'r')\n",
        "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "next(spamreader, None)  # skip the headers\n",
        "ascii_offset = 161\n",
        "\n",
        "lastcase = ''\n",
        "line = ''\n",
        "firstLine = True\n",
        "lines = []\n",
        "caseids = []\n",
        "timeseqs = []\n",
        "timeseqs2 = []\n",
        "times = []\n",
        "times2 = []\n",
        "numlines = 0\n",
        "casestarttime = None\n",
        "lasteventtime = None\n",
        "for row in spamreader:\n",
        "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
        "    if row[0]!=lastcase:\n",
        "        caseids.append(row[0])\n",
        "        casestarttime = t\n",
        "        lasteventtime = t\n",
        "        lastcase = row[0]\n",
        "        if not firstLine:        \n",
        "            lines.append(line)\n",
        "            timeseqs.append(times)\n",
        "            timeseqs2.append(times2)\n",
        "        line = ''\n",
        "        times = []\n",
        "        numlines+=1\n",
        "    line+=unichr(int(row[1])+ascii_offset)\n",
        "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
        "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
        "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
        "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
        "    times.append(timediff)\n",
        "    times2.append(timediff2)\n",
        "    lasteventtime = t\n",
        "    firstLine = False\n",
        "\n",
        "# add last case\n",
        "lines.append(line)\n",
        "timeseqs.append(times)\n",
        "timeseqs2.append(times2)\n",
        "numlines+=1\n",
        "\n",
        "divisor = np.mean([item for sublist in timeseqs for item in sublist])\n",
        "print('divisor: {}'.format(divisor))\n",
        "divisor2 = np.mean([item for sublist in timeseqs2 for item in sublist])\n",
        "print('divisor2: {}'.format(divisor2))\n",
        "\n",
        "elems_per_fold = int(round(numlines/3))\n",
        "fold1 = lines[:elems_per_fold]\n",
        "fold1_c = caseids[:elems_per_fold]\n",
        "fold1_t = timeseqs[:elems_per_fold]\n",
        "fold1_t2 = timeseqs2[:elems_per_fold]\n",
        "\n",
        "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
        "fold2_c = caseids[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
        "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
        "\n",
        "lines = fold1 + fold2\n",
        "caseids = fold1_c + fold2_c\n",
        "lines_t = fold1_t + fold2_t\n",
        "lines_t2 = fold1_t2 + fold2_t2\n",
        "\n",
        "step = 1\n",
        "sentences = []\n",
        "softness = 0\n",
        "next_chars = []\n",
        "lines = map(lambda x: x+'!',lines)\n",
        "maxlen = max(map(lambda x: len(x),lines))\n",
        "\n",
        "chars = map(lambda x : set(x),lines)\n",
        "chars = list(set().union(*chars))\n",
        "chars.sort()\n",
        "target_chars = copy.copy(chars)\n",
        "chars.remove('!')\n",
        "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
        "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
        "print(\"indices_char\")\n",
        "print(indices_char)\n",
        "print(\"target_indices_char\")\n",
        "print(target_indices_char)\n",
        "\n",
        "\n",
        "lastcase = ''\n",
        "line = ''\n",
        "firstLine = True\n",
        "lines = []\n",
        "caseids = []\n",
        "timeseqs = []  # relative time since previous event\n",
        "timeseqs2 = [] # relative time since case start\n",
        "timeseqs3 = [] # absolute time of previous event\n",
        "times = []\n",
        "times2 = []\n",
        "times3 = []\n",
        "numlines = 0\n",
        "casestarttime = None\n",
        "lasteventtime = None\n",
        "csvfile = open('/content/drive/MyDrive/content/data/%s' % eventlog, 'r')\n",
        "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "next(spamreader, None)  # skip the headers\n",
        "for row in spamreader:\n",
        "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
        "    if row[0]!=lastcase:\n",
        "        caseids.append(row[0])\n",
        "        casestarttime = t\n",
        "        lasteventtime = t\n",
        "        lastcase = row[0]\n",
        "        if not firstLine:        \n",
        "            lines.append(line)\n",
        "            timeseqs.append(times)\n",
        "            timeseqs2.append(times2)\n",
        "            timeseqs3.append(times3)\n",
        "        line = ''\n",
        "        times = []\n",
        "        numlines+=1\n",
        "    line+=unichr(int(row[1])+ascii_offset)\n",
        "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
        "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
        "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
        "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
        "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
        "    #timediff = log(timediff+1)\n",
        "    times.append(timediff)\n",
        "    times2.append(timediff2)\n",
        "    times3.append(datetime.fromtimestamp(time.mktime(t)))\n",
        "    lasteventtime = t\n",
        "    firstLine = False\n",
        "\n",
        "# add last case\n",
        "lines.append(line)\n",
        "timeseqs.append(times)\n",
        "timeseqs2.append(times2)\n",
        "timeseqs3.append(times3)\n",
        "numlines+=1\n",
        "\n",
        "fold3 = lines[2*elems_per_fold:]\n",
        "fold3_c = caseids[2*elems_per_fold:]\n",
        "fold3_t = timeseqs[2*elems_per_fold:]\n",
        "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
        "fold3_t3 = timeseqs3[2*elems_per_fold:]\n",
        "#fold3_t4 = timeseqs4[2*elems_per_fold:]\n",
        "\n",
        "lines = fold3\n",
        "caseids = fold3_c\n",
        "lines_t = fold3_t\n",
        "lines_t2 = fold3_t2\n",
        "lines_t3 = fold3_t3\n",
        "#lines_t4 = fold1_t4 + fold2_t4\n",
        "\n",
        "# set parameters\n",
        "predict_size = 1\n",
        "\n",
        "# load model, set this to the model generated by train.py\n",
        "model = load_model('/content/drive/MyDrive/content/code/output_files/models/model_39-271.76.h5') #Ook hier misschien een manier zoeken om het model niet handmatig te moeten ingeven\n",
        "\n",
        "# define helper functions\n",
        "def encode(sentence, times, times3, maxlen=maxlen):\n",
        "    num_features = len(chars)+5\n",
        "    X = np.zeros((1, maxlen, num_features), dtype=np.float32)\n",
        "    leftpad = maxlen-len(sentence)\n",
        "    times2 = np.cumsum(times)\n",
        "    for t, char in enumerate(sentence):\n",
        "        midnight = times3[t].replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "        timesincemidnight = times3[t]-midnight\n",
        "        multiset_abstraction = Counter(sentence[:t+1])\n",
        "        for c in chars:\n",
        "            if c==char:\n",
        "                X[0, t+leftpad, char_indices[c]] = 1\n",
        "        X[0, t+leftpad, len(chars)] = t+1\n",
        "        X[0, t+leftpad, len(chars)+1] = times[t]/divisor\n",
        "        X[0, t+leftpad, len(chars)+2] = times2[t]/divisor2\n",
        "        X[0, t+leftpad, len(chars)+3] = timesincemidnight.seconds/86400\n",
        "        X[0, t+leftpad, len(chars)+4] = times3[t].weekday()/7\n",
        "    return X\n",
        "\n",
        "def getSymbol(predictions):\n",
        "    maxPrediction = 0\n",
        "    symbol = ''\n",
        "    i = 0;\n",
        "    for prediction in predictions:\n",
        "        if(prediction>=maxPrediction):\n",
        "            maxPrediction = prediction\n",
        "            symbol = target_indices_char[i]\n",
        "        i += 1\n",
        "    return symbol\n",
        "\n",
        "one_ahead_gt = []\n",
        "one_ahead_pred = []\n",
        "\n",
        "two_ahead_gt = []\n",
        "two_ahead_pred = []\n",
        "\n",
        "three_ahead_gt = []\n",
        "three_ahead_pred = []\n",
        "\n",
        "\n",
        "# make predictions\n",
        "with open('/content/drive/MyDrive/content/code/output_files/results/next_activity_and_time_%s' % eventlog, 'wb') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    spamwriter.writerow([\"CaseID\", \"Prefix length\", \"Groud truth\", \"Predicted\", \"Levenshtein\", \"Damerau\", \"Jaccard\", \"Ground truth times\", \"Predicted times\", \"RMSE\", \"MAE\"])\n",
        "    for prefix_size in range(2,maxlen):\n",
        "        print(prefix_size)\n",
        "        for line, caseid, times, times3 in izip(lines, caseids, lines_t, lines_t3):\n",
        "            times.append(0)\n",
        "            cropped_line = ''.join(line[:prefix_size])\n",
        "            cropped_times = times[:prefix_size]\n",
        "            cropped_times3 = times3[:prefix_size]\n",
        "            if '!' in cropped_line:\n",
        "                continue # make no prediction for this case, since this case has ended already\n",
        "            ground_truth = ''.join(line[prefix_size:prefix_size+predict_size])\n",
        "            ground_truth_t = times[prefix_size:prefix_size+predict_size]\n",
        "            predicted = ''\n",
        "            predicted_t = []\n",
        "            for i in range(predict_size):\n",
        "                if len(ground_truth)<=i:\n",
        "                    continue\n",
        "                enc = encode(cropped_line, cropped_times, cropped_times3)\n",
        "                y = model.predict(enc, verbose=0)\n",
        "                y_char = y[0][0]\n",
        "                y_t = y[1][0][0]\n",
        "                prediction = getSymbol(y_char)              \n",
        "                cropped_line += prediction\n",
        "                if y_t<0:\n",
        "                    y_t=0\n",
        "                cropped_times.append(y_t)\n",
        "                y_t = y_t * divisor\n",
        "                cropped_times3.append(cropped_times3[-1] + timedelta(seconds=y_t))\n",
        "                predicted_t.append(y_t)\n",
        "                if i==0:\n",
        "                    if len(ground_truth_t)>0:\n",
        "                        one_ahead_pred.append(y_t)\n",
        "                        one_ahead_gt.append(ground_truth_t[0])\n",
        "                if i==1:\n",
        "                    if len(ground_truth_t)>1:\n",
        "                        two_ahead_pred.append(y_t)\n",
        "                        two_ahead_gt.append(ground_truth_t[1])\n",
        "                if i==2:\n",
        "                    if len(ground_truth_t)>2:\n",
        "                        three_ahead_pred.append(y_t)\n",
        "                        three_ahead_gt.append(ground_truth_t[2])\n",
        "                if prediction == '!': # end of case was just predicted, therefore, stop predicting further into the future\n",
        "                    print('! predicted, end case')\n",
        "                    break\n",
        "                predicted += prediction\n",
        "            output = []\n",
        "            if len(ground_truth)>0:\n",
        "                output.append(caseid)\n",
        "                output.append(prefix_size)\n",
        "                output.append(unicode(ground_truth).encode(\"utf-8\"))\n",
        "                output.append(unicode(predicted).encode(\"utf-8\"))\n",
        "                output.append(1 - distance.nlevenshtein(predicted, ground_truth))\n",
        "                dls = 1 - (damerau_levenshtein_distance(unicode(predicted), unicode(ground_truth)) / max(len(predicted),len(ground_truth)))\n",
        "                if dls<0:\n",
        "                    dls=0 # we encountered problems with Damerau-Levenshtein Similarity on some linux machines where the default character encoding of the operating system caused it to be negative, this should never be the case\n",
        "                output.append(dls)\n",
        "                output.append(1 - distance.jaccard(predicted, ground_truth))\n",
        "                output.append('; '.join(str(x) for x in ground_truth_t))\n",
        "                output.append('; '.join(str(x) for x in predicted_t))\n",
        "                if len(predicted_t)>len(ground_truth_t): # if predicted more events than length of case, only use needed number of events for time evaluation\n",
        "                    predicted_t = predicted_t[:len(ground_truth_t)]\n",
        "                if len(ground_truth_t)>len(predicted_t): # if predicted less events than length of case, put 0 as placeholder prediction\n",
        "                    predicted_t.extend(range(len(ground_truth_t)-len(predicted_t)))\n",
        "                if len(ground_truth_t)>0 and len(predicted_t)>0:\n",
        "                    output.append('')\n",
        "                    output.append(metrics.mean_absolute_error([ground_truth_t[0]], [predicted_t[0]]))\n",
        "                    #output.append(metrics.median_absolute_error([ground_truth_t[0]], [predicted_t[0]]))\n",
        "                else:\n",
        "                    output.append('')\n",
        "                    output.append('')\n",
        "                    output.append('')\n",
        "                spamwriter.writerow(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting distance\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n",
            "\r\u001b[K     |                              | 10kB 26.2MB/s eta 0:00:01\r\u001b[K     |                            | 20kB 31.3MB/s eta 0:00:01\r\u001b[K     |                          | 30kB 25.3MB/s eta 0:00:01\r\u001b[K     |                        | 40kB 22.6MB/s eta 0:00:01\r\u001b[K     |                       | 51kB 24.2MB/s eta 0:00:01\r\u001b[K     |                     | 61kB 17.0MB/s eta 0:00:01\r\u001b[K     |                   | 71kB 17.7MB/s eta 0:00:01\r\u001b[K     |                 | 81kB 18.2MB/s eta 0:00:01\r\u001b[K     |               | 92kB 16.4MB/s eta 0:00:01\r\u001b[K     |             | 102kB 17.6MB/s eta 0:00:01\r\u001b[K     |            | 112kB 17.6MB/s eta 0:00:01\r\u001b[K     |          | 122kB 17.6MB/s eta 0:00:01\r\u001b[K     |        | 133kB 17.6MB/s eta 0:00:01\r\u001b[K     |      | 143kB 17.6MB/s eta 0:00:01\r\u001b[K     |    | 153kB 17.6MB/s eta 0:00:01\r\u001b[K     |   | 163kB 17.6MB/s eta 0:00:01\r\u001b[K     | | 174kB 17.6MB/s eta 0:00:01\r\u001b[K     || 184kB 17.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-cp27-none-any.whl size=16262 sha256=606d47bdb47047ed86d25ef1dc18feeaa619dee1ac81a755ef2fc1ad8fb5ee92\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/aa/e1/dbba9e7b6d397d645d0f12db1c66dbae9c5442b39b001db18e\n",
            "Successfully built distance\n",
            "Installing collected packages: distance\n",
            "Successfully installed distance-0.1.3\n",
            "Collecting jellyfish\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/3f/60ac86fb43dfbf976768e80674b5538e535f6eca5aa7806cf2fdfd63550f/jellyfish-0.6.1.tar.gz (132kB)\n",
            "\u001b[K     || 133kB 17.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.6.1-cp27-cp27mu-linux_x86_64.whl size=46404 sha256=5f5123ca08babe1df8877cb933400000faa33a7928177105bfd27191d31e51a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/6f/33/92bb9a4b4562a60ba6a80cedbab8907e48bc7a8b1f369ea0ae\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: jellyfish\n",
            "Successfully installed jellyfish-0.6.1\n",
            "Collecting unicodecsv\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-cp27-none-any.whl size=10768 sha256=947ec717bb815fa902c22851f91248b50bf8e57cdd50087b9e208ef9242ea93d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv\n",
            "Successfully installed unicodecsv-0.14.1\n",
            "Requirement already satisfied: distance in /usr/local/lib/python2.7/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "divisor: 164.004640656\n",
            "divisor2: 6031.62953937\n",
            "total chars: 23, target chars: 24\n",
            "indices_char\n",
            "{0: u'\\xa2', 1: u'\\xa3', 2: u'\\xa4', 3: u'\\xa5', 4: u'\\xa6', 5: u'\\xa7', 6: u'\\xa8', 7: u'\\xa9', 8: u'\\xaa', 9: u'\\xab', 10: u'\\xac', 11: u'\\xad', 12: u'\\xae', 13: u'\\xaf', 14: u'\\xb0', 15: u'\\xb1', 16: u'\\xb2', 17: u'\\xb3', 18: u'\\xb4', 19: u'\\xb5', 20: u'\\xb6', 21: u'\\xb7', 22: u'\\xb8'}\n",
            "target_indices_char\n",
            "{0: u'!', 1: u'\\xa2', 2: u'\\xa3', 3: u'\\xa4', 4: u'\\xa5', 5: u'\\xa6', 6: u'\\xa7', 7: u'\\xa8', 8: u'\\xa9', 9: u'\\xaa', 10: u'\\xab', 11: u'\\xac', 12: u'\\xad', 13: u'\\xae', 14: u'\\xaf', 15: u'\\xb0', 16: u'\\xb1', 17: u'\\xb2', 18: u'\\xb3', 19: u'\\xb4', 20: u'\\xb5', 21: u'\\xb6', 22: u'\\xb7', 23: u'\\xb8'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8fe8f23eea3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;31m# load model, set this to the model generated by train.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/content/code/output_files/models/model_39-271.76.h5'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Ook hier misschien een manier zoeken om het model niet handmatig te moeten ingeven\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;31m# define helper functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/saving.pyc\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/saving.pyc\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/saving.pyc\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(h5dict, custom_objects, compile)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/saving.pyc\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    625\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/__init__.pyc\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 147\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   1073\u001b[0m                         \u001b[0mnode_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_data_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m                             \u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m                         \u001b[0;31m# If the node does not have all inbound layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;31m# and building the layer if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# set or validate state_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1917\u001b[0m                                       \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1919\u001b[0;31m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[1;32m   1920\u001b[0m         self.recurrent_kernel = self.add_weight(\n\u001b[1;32m   1921\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         weight = K.variable(initializer(shape, dtype=dtype),\n\u001b[0m\u001b[1;32m    280\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/initializers.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             x = K.random_uniform(shape, -limit, limit,\n\u001b[0;32m--> 227\u001b[0;31m                                  dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m   4355\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         return tf_keras_backend.random_uniform(\n\u001b[0;32m-> 4357\u001b[0;31m             shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/backend.pyc\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m   5624\u001b[0m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5625\u001b[0m   return random_ops.random_uniform(\n\u001b[0;32m-> 5626\u001b[0;31m       shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n\u001b[0m\u001b[1;32m   5627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/random_ops.pyc\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m           shape, minval, maxval, seed=seed1, seed2=seed2, name=name)\n\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_random_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m# TODO(b/132092188): C++ shape inference inside functional ops does not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gen_random_ops.pyc\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[1;32m    713\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m    714\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RandomUniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         tld.op_callbacks, shape, \"seed\", seed, \"seed2\", seed2, \"dtype\", dtype)\n\u001b[0m\u001b[1;32m    716\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swpGJDzYVcGA"
      },
      "source": [
        "# Extracting folds in numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbUoR8wuVlPz",
        "outputId": "d26508c0-f10a-4f17-af47-267d9bf740aa"
      },
      "source": [
        "!pip install unicodecsv\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import unicodecsv\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "csvfile = open('/content/drive/MyDrive/content/data/%s' % eventlog, 'r')\n",
        "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "next(spamreader, None)  # skip the headers\n",
        "lastcase = ''\n",
        "line = ''\n",
        "firstLine = True\n",
        "lines = []\n",
        "timeseqs = []\n",
        "timeseqs2 = []\n",
        "timeseqs3 = []\n",
        "timeseqs4 = []\n",
        "times = []\n",
        "times2 = []\n",
        "times3 = []\n",
        "times4 = []\n",
        "numlines = 0\n",
        "casestarttime = None\n",
        "lasteventtime = None\n",
        "for row in spamreader:\n",
        "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
        "    if row[0]!=lastcase:\n",
        "        casestarttime = t\n",
        "        lasteventtime = t\n",
        "        lastcase = row[0]\n",
        "        if not firstLine:\n",
        "            lines.append(line)\n",
        "            timeseqs.append(times)\n",
        "            timeseqs2.append(times2)\n",
        "            timeseqs3.append(times3)\n",
        "            timeseqs4.append(times4)\n",
        "        line = ''\n",
        "        times = []\n",
        "        times2 = []\n",
        "        times3 = []\n",
        "        times4 = []\n",
        "        numlines+=1\n",
        "    if int(row[1])>=10:\n",
        "      line+=str(row[1])\n",
        "    else:\n",
        "      line+=str(0)+str(row[1])\n",
        "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
        "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
        "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
        "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
        "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
        "    timediff3 = timesincemidnight.seconds #this leaves only time even occured after midnight\n",
        "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\n",
        "    times.append(timediff)\n",
        "    times2.append(timediff2)\n",
        "    times3.append(timediff3)\n",
        "    times4.append(timediff4)\n",
        "    lasteventtime = t\n",
        "    firstLine = False\n",
        " \n",
        "# add last case\n",
        "lines.append(line)\n",
        "timeseqs.append(times)\n",
        "timeseqs2.append(times2)\n",
        "timeseqs3.append(times3)\n",
        "timeseqs4.append(times4)\n",
        "numlines+=1\n",
        "\n",
        "print(pd.DataFrame(lines))\n",
        "\n",
        "elems_per_fold = int(round(numlines/3))\n",
        "print(\"elements per forld: \"+ str(elems_per_fold))\n",
        "fold1 = lines[:elems_per_fold]\n",
        "pd.DataFrame(fold1).to_csv('/content/drive/MyDrive/content/code/output_files/folds/fold1num.csv')\n",
        "print(pd.DataFrame(fold1))\n",
        "\n",
        "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
        "pd.DataFrame(fold2).to_csv('/content/drive/MyDrive/content/code/output_files/folds/fold2num.csv')\n",
        "print(pd.DataFrame(fold2))\n",
        "\n",
        "fold3 = lines[2*elems_per_fold:]\n",
        "pd.DataFrame(fold3).to_csv('/content/drive/MyDrive/content/code/output_files/folds/fold3num.csv')\n",
        "print(pd.DataFrame(fold3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unicodecsv in /usr/local/lib/python2.7/dist-packages (0.14.1)\n",
            "                                                      0\n",
            "0            070803171707081718192205081718192018192208\n",
            "1     1703170508070817031703171823131414231315050807...\n",
            "2                                  05080208020508030303\n",
            "3                                        02020208020208\n",
            "4     0208132314150213142313142323141313231405080208...\n",
            "5            020802030303030302080208020317171717030308\n",
            "6     0203020802050805080508041705080508050805080317...\n",
            "7                          0205080417182119181922041708\n",
            "8     0208021323141323141323141323141323142313141323...\n",
            "9     0208020417182119182018201818202020181820191820...\n",
            "10    1702080208021708020208170805020802040417182121...\n",
            "11                           02080203030303030303030303\n",
            "12                                       02040403040208\n",
            "13                                           0208021708\n",
            "14                                         021708020408\n",
            "15                                               201819\n",
            "16    1323141413231523131413231413231413231415132314...\n",
            "17                                 02020223131405080208\n",
            "18    0205080417040517080405041708070817030303171705...\n",
            "19                   0205080204170508080708170617030208\n",
            "20                                               031719\n",
            "21    0204041705080508020202080217211819050805080208...\n",
            "22                                       02040805020802\n",
            "23                                   020208020508020208\n",
            "24                                     0208020708170208\n",
            "25    0317171717182119181820201818201918191820201820...\n",
            "26    0202080217050817021704041717041704050804050817...\n",
            "27                               0202080208020208021102\n",
            "28                                       02141323050808\n",
            "29    0203132314042314131323142313141504132314031503...\n",
            "...                                                 ...\n",
            "3465  0502021323141323141314231323142313142313142313...\n",
            "3466       02020317170318201818201820201820181918190202\n",
            "3467  0217060705031703172018201820181918202018182020...\n",
            "3468               020202040502020303030317171717030302\n",
            "3469  0205080317171819080317170508131423231314132314...\n",
            "3470                                   0508050817040508\n",
            "3471       05080708132314132314231413132314231314132314\n",
            "3472                                           05080208\n",
            "3473                                             050808\n",
            "3474  0508050805080317031705080708031717031819031705...\n",
            "3475  0505080508031705080202081323142313141323141513...\n",
            "3476  0508020802080217050817041717031820182018182020...\n",
            "3477  0204031718191819080305170805050804040404040404...\n",
            "3478                                           02040208\n",
            "3479  0204132314141323131423040406040508020802171821...\n",
            "3480  1323140404060604042313140404030402080508020805...\n",
            "3481                                             192208\n",
            "3482  0203171717170406060607081705080708060708170606...\n",
            "3483  0207080508030317182118201918191819181918191819...\n",
            "3484                                         0203171819\n",
            "3485  1820181918191819181918191819181918221918201820...\n",
            "3486                                   0413230414050802\n",
            "3487                             0205080417041705080208\n",
            "3488                                             020502\n",
            "3489  0205040417050417181903170318191819201819181918...\n",
            "3490  0205080202080213231414132313231413231413231414...\n",
            "3491           0217171717030802050804132314040313231408\n",
            "3492  0203070805080708050813231413231413231404132314...\n",
            "3493  0208021323142313141323141323141314231323142314...\n",
            "3494                                         0208020808\n",
            "\n",
            "[3495 rows x 1 columns]\n",
            "                                                      0\n",
            "0                                              04170508\n",
            "1     0208021323141323142313141314230508020217040508...\n",
            "2                    0813231405080805050802041704170508\n",
            "3                    2313142313142313142313142313140508\n",
            "4     0208021323140413142304061204231314121323140423...\n",
            "5                                  17020802080208020802\n",
            "6     0208030305080403030508040404041704050804050811...\n",
            "7     1703051708150508050805080317170305080508040303...\n",
            "8     0508170303030303170317170305170802050804050803...\n",
            "9     0508050817050817030317031703170303171703170303...\n",
            "10    0413231413231423131415132314131423040303030303...\n",
            "11    1704041718201822191804171819182119182120182018...\n",
            "12    0217182121221904171821211918221904171821211918...\n",
            "13    0202020823131413231404132314020203020802081323...\n",
            "14                                         021704170208\n",
            "15    0213142323131404131423132314131423040303030323...\n",
            "16    0217182018201818202020181818202219041718212119...\n",
            "17                                             02080808\n",
            "18                           02020802080202020802080202\n",
            "19                                             02020802\n",
            "20                           02020802030803030803080303\n",
            "21                       080208020403030303030303030308\n",
            "22    0204040508040404080202060708020802040508050802...\n",
            "23    1702080204020802040606061314230404042323232323...\n",
            "24                                               170208\n",
            "25                                             05080208\n",
            "26                       022313142314131314232313141508\n",
            "27    0213231413231415141323131423131423142313050817...\n",
            "28             0217031703171819181918182020182018182019\n",
            "29                               1702081708081720181918\n",
            "...                                                 ...\n",
            "1135  0502021323141323141314231323142313142313142313...\n",
            "1136       02020317170318201818201820201820181918190202\n",
            "1137  0217060705031703172018201820181918202018182020...\n",
            "1138               020202040502020303030317171717030302\n",
            "1139  0205080317171819080317170508131423231314132314...\n",
            "1140                                   0508050817040508\n",
            "1141       05080708132314132314231413132314231314132314\n",
            "1142                                           05080208\n",
            "1143                                             050808\n",
            "1144  0508050805080317031705080708031717031819031705...\n",
            "1145  0505080508031705080202081323142313141323141513...\n",
            "1146  0508020802080217050817041717031820182018182020...\n",
            "1147  0204031718191819080305170805050804040404040404...\n",
            "1148                                           02040208\n",
            "1149  0204132314141323131423040406040508020802171821...\n",
            "1150  1323140404060604042313140404030402080508020805...\n",
            "1151                                             192208\n",
            "1152  0203171717170406060607081705080708060708170606...\n",
            "1153  0207080508030317182118201918191819181918191819...\n",
            "1154                                         0203171819\n",
            "1155  1820181918191819181918191819181918221918201820...\n",
            "1156                                   0413230414050802\n",
            "1157                             0205080417041705080208\n",
            "1158                                             020502\n",
            "1159  0205040417050417181903170318191819201819181918...\n",
            "1160  0205080202080213231414132313231413231413231414...\n",
            "1161           0217171717030802050804132314040313231408\n",
            "1162  0203070805080708050813231413231413231404132314...\n",
            "1163  0208021323142313141323141323141314231323142314...\n",
            "1164                                         0208020808\n",
            "\n",
            "[1165 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "yZ_s8JFcXfqK",
        "outputId": "c6f52f8e-cb9a-428a-9964-69d1b9701d62"
      },
      "source": [
        "import pandas as pd\n",
        "print(\n",
        "    str(0)+str(5)\n",
        ")\n",
        "print(int(str(0)+str(5)))\n",
        "\n",
        "df = pd.DataFrame(columns=[\"truth\", \"predicted\"])\n",
        "print(df)\n",
        "df = df.iloc[0,0]=0\n",
        "df = df.iloc[0,1]=21\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05\n",
            "5\n",
            "Empty DataFrame\n",
            "Columns: [truth, predicted]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-532257d3ef51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"truth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predicted\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'iloc'"
          ]
        }
      ]
    }
  ]
}